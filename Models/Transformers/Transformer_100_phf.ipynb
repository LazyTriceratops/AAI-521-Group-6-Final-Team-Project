{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6faJMsrrUKr-"
   },
   "source": [
    "# Transformer Model Pose Hands Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5572,
     "status": "ok",
     "timestamp": 1733455195265,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "mmz_eqr4SycP"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19819,
     "status": "ok",
     "timestamp": 1733455215075,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "BtxEwPWUUguy",
    "outputId": "59363d00-502d-4537-e1ac-b4e3d00bbce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7381,
     "status": "ok",
     "timestamp": 1733455222454,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "AOXIh31bfq_w"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/Models/MediaPipe_processed'\n",
    "NUM_FRAMES = 90\n",
    "save_path = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/final_DataSet'\n",
    "\n",
    "# Load the top 100 glosses\n",
    "top_100_path = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/DataSet/gloss_counts_top_100.csv'\n",
    "df = pd.read_csv(top_100_path)\n",
    "top_100_classes = df['Gloss'].tolist()  # List of the top 100 glosses\n",
    "\n",
    "# Label map for only top 100 glosses\n",
    "actions = sorted(os.listdir(DATA_PATH))\n",
    "label_map = {label: idx for idx, label in enumerate(actions) if label in top_100_classes}\n",
    "\n",
    "# Ensure the label map only contains the top 100\n",
    "label_map = {label: idx for idx, label in enumerate(top_100_classes)}  # Recreate the label map for top 100 only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EBvwIPkSGq3"
   },
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    try:\n",
    "        # Get the gloss label from the file path\n",
    "        label = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "        # Skip files whose labels are not in the top 100\n",
    "        if label not in top_100_classes:\n",
    "            return None\n",
    "\n",
    "        # Load .npy file\n",
    "        sequence = np.load(file_path)\n",
    "\n",
    "        # Normalize keypoints\n",
    "        sequence = sequence / np.max(np.abs(sequence), axis=(0, 1), keepdims=True)\n",
    "\n",
    "        # Pad or truncate to NUM_FRAMES\n",
    "        return sequence[:NUM_FRAMES] if len(sequence) > NUM_FRAMES else np.pad(\n",
    "            sequence, ((0, NUM_FRAMES - len(sequence)), (0, 0)), 'constant'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None  # Skip this file\n",
    "\n",
    "# Count total files and prepare paths, filtering by top 100 glosses\n",
    "all_files = [\n",
    "    os.path.join(DATA_PATH, action, file)\n",
    "    for action in os.listdir(DATA_PATH)\n",
    "    if action in top_100_classes  # Only include glosses from the top 100\n",
    "    for file in os.listdir(os.path.join(DATA_PATH, action))\n",
    "    if file.endswith('.npy')\n",
    "]\n",
    "\n",
    "# Process files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_file, all_files), total=len(all_files), desc=\"Processing Files\"))\n",
    "\n",
    "# Filter out None results\n",
    "sequences = [seq for seq in results if seq is not None]\n",
    "labels = [\n",
    "    label_map[os.path.basename(os.path.dirname(file))] for file, seq in zip(all_files, results) if seq is not None\n",
    "]\n",
    "\n",
    "# Check if any label exceeds the number of classes\n",
    "assert all(label < len(label_map) for label in labels), \"Some labels are out of bounds!\"\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)  # Shape: (num_samples, NUM_FRAMES, num_features)\n",
    "y = to_categorical(labels, num_classes=len(label_map))  # Shape: (num_samples, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21161,
     "status": "ok",
     "timestamp": 1733417074335,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "m5XGVZXkf8O0",
    "outputId": "0b161fcb-46f0-4701-94b7-17f68b7246c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and y saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save X and y arrays\n",
    "np.save(os.path.join(save_path, 'X_mp_phf_100.npy'), X)\n",
    "np.save(os.path.join(save_path, 'y_mp_phf_100.npy'), y)\n",
    "\n",
    "print(\"X and y saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import itertools\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.layers import Input, MultiHeadAttention, Dense, Dropout, LayerNormalization, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2  # Import l2 regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 27417,
     "status": "ok",
     "timestamp": 1733455249862,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "9CyvO2FsfPX8"
   },
   "outputs": [],
   "source": [
    "# Load processed data from disk\n",
    "save_path = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/final_DataSet'\n",
    "\n",
    "X = np.load(os.path.join(save_path, 'X_mp_phf_100.npy'))\n",
    "y = np.load(os.path.join(save_path, 'y_mp_phf_100.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1733455250332,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "vb9-CTcbfMZQ",
    "outputId": "10ffab91-4149-4bed-ea93-71b28a95b866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (896, 90, 1662), y_train shape: (896, 100)\n",
      "X_val shape: (112, 90, 1662), y_val shape: (112, 100)\n",
      "X_test shape: (112, 90, 1662), y_test shape: (112, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train-test-validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1733455254828,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "El80ZDqcuDga"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_blocks': [1, 2],  # Reduce number of transformer blocks\n",
    "    'ff_dim': [32, 64],  # Reduce feed-forward dimension\n",
    "    'batch_size': [8, 16],  # Reduce batch size\n",
    "    'learning_rate': [1e-6, 1e-5]  # Lower learning rates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1733455258858,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "FmYQaocPgJru",
    "outputId": "fbe8e4f5-3db4-431b-8dec-98643391fed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 1\n",
      "GPU Name: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"GPUs available: {len(physical_devices)}\")\n",
    "    for gpu in physical_devices:\n",
    "        print(f\"GPU Name: {gpu.name}\")\n",
    "    # Set memory growth BEFORE any TensorFlow operations\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1733455261343,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "7dvw35AViv2k"
   },
   "outputs": [],
   "source": [
    "# Transformer Block with Batch Normalization and L2 Regularization\n",
    "def transformer_block(inputs, num_heads, ff_dim, dropout=0.1, l2_reg=1e-4):\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "    ff = Dense(ff_dim, activation=\"relu\", kernel_regularizer=l2(l2_reg))(attention)\n",
    "    ff = Dense(inputs.shape[-1], kernel_regularizer=l2(l2_reg))(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = BatchNormalization()(ff)  # Add BatchNormalization\n",
    "    outputs = LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1733455263655,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "81KetHOsjzvd"
   },
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def add_positional_encoding(inputs):\n",
    "    seq_len = inputs.shape[1]  # Sequence length\n",
    "    dim = inputs.shape[-1]  # Feature size\n",
    "    pos_enc = np.array([[pos / np.power(10000, 2 * (i // 2) / dim) for i in range(dim)] for pos in range(seq_len)])\n",
    "    pos_enc[:, 0::2] = np.sin(pos_enc[:, 0::2])  # Apply sin to even indices\n",
    "    pos_enc[:, 1::2] = np.cos(pos_enc[:, 1::2])  # Apply cos to odd indices\n",
    "\n",
    "    pos_enc = tf.constant(pos_enc, dtype=tf.float32)\n",
    "    pos_enc = tf.expand_dims(pos_enc, axis=0)  # Add batch dimension\n",
    "\n",
    "    return inputs + pos_enc  # Add positional encoding to the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1733455265842,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "NL4nfIhbizDg"
   },
   "outputs": [],
   "source": [
    "# Function to build and compile the model\n",
    "def build_transformer_model(seq_len, num_features, num_classes, num_heads=4, ff_dim=128, num_blocks=3, dropout=0.1, learning_rate=1e-4):\n",
    "    inputs = Input(shape=(seq_len, num_features))\n",
    "    x = add_positional_encoding(inputs)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2309,
     "status": "ok",
     "timestamp": 1733455270034,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "nSapBYCai3Es",
    "outputId": "4665936e-a334-40c9-b42c-747acd85242f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">44,217,510</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                           │                        │                │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,324</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,432</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">108,030</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,648</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                           │                        │                │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,324</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">44,217,510</span> │ layer_normalization_1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                           │                        │                │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,324</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,432</span> │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">108,030</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,648</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,324</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1662</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_3… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">851,456</span> │ global_average_poolin… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │     \u001b[38;5;34m44,217,510\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                           │                        │                │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m3,324\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m106,432\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │        \u001b[38;5;34m108,030\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m6,648\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                           │                        │                │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m3,324\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │     \u001b[38;5;34m44,217,510\u001b[0m │ layer_normalization_1… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                           │                        │                │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m3,324\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m106,432\u001b[0m │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │        \u001b[38;5;34m108,030\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m6,648\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m1662\u001b[0m)       │          \u001b[38;5;34m3,324\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1662\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_3… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m851,456\u001b[0m │ global_average_poolin… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m51,300\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,793,292</span> (342.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,793,292\u001b[0m (342.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,786,644</span> (342.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m89,786,644\u001b[0m (342.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,648</span> (25.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,648\u001b[0m (25.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning Rate Schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-5,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# AdamW Optimizer (with weight decay)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "\n",
    "# Model Parameters\n",
    "seq_len = 90\n",
    "num_features = 1662\n",
    "num_classes = 100\n",
    "num_heads = 4\n",
    "ff_dim = 64\n",
    "num_blocks = 2\n",
    "\n",
    "# Build the Model\n",
    "transformer_model = build_transformer_model(seq_len, num_features, num_classes, num_heads, ff_dim, num_blocks)\n",
    "transformer_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1733455272960,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "Zewv0xgaeaX6"
   },
   "outputs": [],
   "source": [
    "# Function for grid search\n",
    "def grid_search(param_grid, X_train, y_train, X_val, y_val):\n",
    "    best_val_loss = np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_history = None\n",
    "\n",
    "    # Grid search over parameters\n",
    "    for params in itertools.product(*param_grid.values()):\n",
    "        num_blocks, ff_dim, batch_size, learning_rate = params\n",
    "\n",
    "        print(f\"Training with params: num_blocks={num_blocks}, ff_dim={ff_dim}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "\n",
    "        # Build and train the model with the current set of hyperparameters\n",
    "        model = build_transformer_model(seq_len=90, num_features=1662, num_classes=100,\n",
    "                                        num_blocks=num_blocks, ff_dim=ff_dim, learning_rate=learning_rate)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(f\"best_model_{num_blocks}_{ff_dim}_{batch_size}_{learning_rate}.keras\",\n",
    "                                           save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.arange(len(label_map)),\n",
    "            y=np.argmax(y_train, axis=1)\n",
    "        )\n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[early_stopping, model_checkpoint],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Check validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            best_history = history\n",
    "            print(f\"New Best Model Found: val_loss={val_loss}, params={params}\")\n",
    "\n",
    "    return best_model, best_history, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2604042,
     "status": "ok",
     "timestamp": 1733457880631,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "KWWkGoHCvNWG",
    "outputId": "4ca3b761-4b43-4b0e-b04a-10079e1837f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: num_blocks=1, ff_dim=32, batch_size=8, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.69529, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.69529 to 4.65654, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.65654 to 4.62664, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.62664 to 4.61587, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.61587 to 4.61490, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.61490 to 4.61116, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.61116\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.61116\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61116\n",
      "\n",
      "Epoch 10: val_loss improved from 4.61116 to 4.61110, saving model to best_model_1_32_8_1e-06.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.61110\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.61110\n",
      "New Best Model Found: val_loss=4.61110258102417, params=(1, 32, 8, 1e-06)\n",
      "Training with params: num_blocks=1, ff_dim=32, batch_size=8, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.65337, saving model to best_model_1_32_8_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 4.65337\n",
      "\n",
      "Epoch 3: val_loss improved from 4.65337 to 4.60751, saving model to best_model_1_32_8_1e-05.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 5: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.60751\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.60751\n",
      "New Best Model Found: val_loss=4.607507228851318, params=(1, 32, 8, 1e-05)\n",
      "Training with params: num_blocks=1, ff_dim=32, batch_size=16, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.81198, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.81198 to 4.74675, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.74675 to 4.70275, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.70275 to 4.67283, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.67283 to 4.65646, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.65646 to 4.63604, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.63604 to 4.62689, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 4.62689 to 4.61186, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.61186\n",
      "\n",
      "Epoch 16: val_loss improved from 4.61186 to 4.60859, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.60859\n",
      "\n",
      "Epoch 24: val_loss improved from 4.60859 to 4.60389, saving model to best_model_1_32_16_1e-06.keras\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 31: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.60389\n",
      "\n",
      "Epoch 34: val_loss did not improve from 4.60389\n",
      "New Best Model Found: val_loss=4.603893756866455, params=(1, 32, 16, 1e-06)\n",
      "Training with params: num_blocks=1, ff_dim=32, batch_size=16, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.67144, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 4.67144\n",
      "\n",
      "Epoch 3: val_loss improved from 4.67144 to 4.64734, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.64734 to 4.62216, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.62216 to 4.61944, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.61944\n",
      "\n",
      "Epoch 7: val_loss improved from 4.61944 to 4.61371, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.61371\n",
      "\n",
      "Epoch 14: val_loss improved from 4.61371 to 4.60914, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.60914\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.60914\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.60914\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.60914\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.60914\n",
      "\n",
      "Epoch 20: val_loss improved from 4.60914 to 4.60552, saving model to best_model_1_32_16_1e-05.keras\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.60552\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.60552\n",
      "Training with params: num_blocks=1, ff_dim=64, batch_size=8, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.75079, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.75079 to 4.70255, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.70255 to 4.67817, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.67817 to 4.66555, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.66555 to 4.65959, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.65959 to 4.65252, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.65252 to 4.64436, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.64436\n",
      "\n",
      "Epoch 9: val_loss improved from 4.64436 to 4.64389, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 4.64389 to 4.63948, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.63948\n",
      "\n",
      "Epoch 12: val_loss improved from 4.63948 to 4.63725, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 4.63725 to 4.63079, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 14: val_loss improved from 4.63079 to 4.62457, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.62457\n",
      "\n",
      "Epoch 22: val_loss improved from 4.62457 to 4.62347, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 4.62347 to 4.62309, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.62309\n",
      "\n",
      "Epoch 31: val_loss improved from 4.62309 to 4.62042, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4.62042\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.62042\n",
      "\n",
      "Epoch 34: val_loss improved from 4.62042 to 4.61246, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 35: val_loss did not improve from 4.61246\n",
      "\n",
      "Epoch 36: val_loss did not improve from 4.61246\n",
      "\n",
      "Epoch 37: val_loss did not improve from 4.61246\n",
      "\n",
      "Epoch 38: val_loss improved from 4.61246 to 4.61069, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 4.61069\n",
      "\n",
      "Epoch 40: val_loss improved from 4.61069 to 4.60734, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 41: val_loss improved from 4.60734 to 4.60576, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 4.60576 to 4.60499, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.60499\n",
      "\n",
      "Epoch 44: val_loss improved from 4.60499 to 4.59961, saving model to best_model_1_64_8_1e-06.keras\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 47: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 48: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 51: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 52: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.59961\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.59961\n",
      "New Best Model Found: val_loss=4.599613666534424, params=(1, 64, 8, 1e-06)\n",
      "Training with params: num_blocks=1, ff_dim=64, batch_size=8, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.66592, saving model to best_model_1_64_8_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.66592 to 4.63455, saving model to best_model_1_64_8_1e-05.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 4.63455\n",
      "\n",
      "Epoch 4: val_loss did not improve from 4.63455\n",
      "\n",
      "Epoch 5: val_loss improved from 4.63455 to 4.63332, saving model to best_model_1_64_8_1e-05.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.63332 to 4.62865, saving model to best_model_1_64_8_1e-05.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.62865\n",
      "\n",
      "Epoch 16: val_loss improved from 4.62865 to 4.61804, saving model to best_model_1_64_8_1e-05.keras\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.61804\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.61804\n",
      "Training with params: num_blocks=1, ff_dim=64, batch_size=16, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.96195, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.96195 to 4.82850, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.82850 to 4.75172, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.75172 to 4.70913, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.70913 to 4.68154, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.68154 to 4.67264, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.67264 to 4.65717, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 4.65717 to 4.65434, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 4.65434 to 4.63833, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.63833\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.63833\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.63833\n",
      "\n",
      "Epoch 13: val_loss improved from 4.63833 to 4.63166, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.63166\n",
      "\n",
      "Epoch 20: val_loss improved from 4.63166 to 4.63144, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.63144\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.63144\n",
      "\n",
      "Epoch 23: val_loss improved from 4.63144 to 4.63042, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 4.63042 to 4.62406, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 4.62406 to 4.62218, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 4.62218 to 4.62082, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.62082\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.62082\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.62082\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.62082\n",
      "\n",
      "Epoch 31: val_loss improved from 4.62082 to 4.61640, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 4.61640 to 4.61634, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.61634\n",
      "\n",
      "Epoch 34: val_loss improved from 4.61634 to 4.61184, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 4.61184 to 4.60513, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 4.60513\n",
      "\n",
      "Epoch 37: val_loss did not improve from 4.60513\n",
      "\n",
      "Epoch 38: val_loss improved from 4.60513 to 4.60288, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 40: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 41: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 42: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 44: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.60288\n",
      "\n",
      "Epoch 46: val_loss improved from 4.60288 to 4.60230, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 47: val_loss improved from 4.60230 to 4.59816, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 48: val_loss improved from 4.59816 to 4.58613, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.58613\n",
      "\n",
      "Epoch 50: val_loss improved from 4.58613 to 4.58591, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 51: val_loss did not improve from 4.58591\n",
      "\n",
      "Epoch 52: val_loss improved from 4.58591 to 4.57854, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.57854\n",
      "\n",
      "Epoch 54: val_loss improved from 4.57854 to 4.57616, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 55: val_loss improved from 4.57616 to 4.57090, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 56: val_loss did not improve from 4.57090\n",
      "\n",
      "Epoch 57: val_loss did not improve from 4.57090\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.57090\n",
      "\n",
      "Epoch 59: val_loss improved from 4.57090 to 4.56699, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 60: val_loss improved from 4.56699 to 4.56171, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 61: val_loss improved from 4.56171 to 4.55748, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 62: val_loss improved from 4.55748 to 4.55367, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 63: val_loss improved from 4.55367 to 4.55032, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 64: val_loss improved from 4.55032 to 4.54377, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.54377\n",
      "\n",
      "Epoch 66: val_loss improved from 4.54377 to 4.54218, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 67: val_loss did not improve from 4.54218\n",
      "\n",
      "Epoch 68: val_loss improved from 4.54218 to 4.53577, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 69: val_loss improved from 4.53577 to 4.52992, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 70: val_loss did not improve from 4.52992\n",
      "\n",
      "Epoch 71: val_loss did not improve from 4.52992\n",
      "\n",
      "Epoch 72: val_loss improved from 4.52992 to 4.52669, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 73: val_loss improved from 4.52669 to 4.51577, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 74: val_loss improved from 4.51577 to 4.50402, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 75: val_loss improved from 4.50402 to 4.50173, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 76: val_loss improved from 4.50173 to 4.49955, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 77: val_loss did not improve from 4.49955\n",
      "\n",
      "Epoch 78: val_loss improved from 4.49955 to 4.49846, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 79: val_loss improved from 4.49846 to 4.48919, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 80: val_loss improved from 4.48919 to 4.48344, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 81: val_loss did not improve from 4.48344\n",
      "\n",
      "Epoch 82: val_loss improved from 4.48344 to 4.47696, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 83: val_loss improved from 4.47696 to 4.47526, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 84: val_loss improved from 4.47526 to 4.46515, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 85: val_loss improved from 4.46515 to 4.46248, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 86: val_loss improved from 4.46248 to 4.46017, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 87: val_loss did not improve from 4.46017\n",
      "\n",
      "Epoch 88: val_loss did not improve from 4.46017\n",
      "\n",
      "Epoch 89: val_loss improved from 4.46017 to 4.45949, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 90: val_loss improved from 4.45949 to 4.45714, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 91: val_loss improved from 4.45714 to 4.45353, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 92: val_loss improved from 4.45353 to 4.44986, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 93: val_loss improved from 4.44986 to 4.44787, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 94: val_loss improved from 4.44787 to 4.43943, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 95: val_loss improved from 4.43943 to 4.43828, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 96: val_loss improved from 4.43828 to 4.43690, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 97: val_loss did not improve from 4.43690\n",
      "\n",
      "Epoch 98: val_loss improved from 4.43690 to 4.43684, saving model to best_model_1_64_16_1e-06.keras\n",
      "\n",
      "Epoch 99: val_loss did not improve from 4.43684\n",
      "\n",
      "Epoch 100: val_loss did not improve from 4.43684\n",
      "New Best Model Found: val_loss=4.436836242675781, params=(1, 64, 16, 1e-06)\n",
      "Training with params: num_blocks=1, ff_dim=64, batch_size=16, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.72107, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.72107 to 4.63995, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 4.63995\n",
      "\n",
      "Epoch 4: val_loss improved from 4.63995 to 4.63956, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 4.63956\n",
      "\n",
      "Epoch 6: val_loss improved from 4.63956 to 4.63663, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.63663 to 4.61931, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.61931\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61931\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.61931\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61931\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61931\n",
      "\n",
      "Epoch 13: val_loss improved from 4.61931 to 4.61670, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.61670\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.61670\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.61670\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.61670\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.61670\n",
      "\n",
      "Epoch 19: val_loss improved from 4.61670 to 4.61373, saving model to best_model_1_64_16_1e-05.keras\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.61373\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.61373\n",
      "Training with params: num_blocks=2, ff_dim=32, batch_size=8, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.80277, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.80277 to 4.72037, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.72037 to 4.67611, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.67611 to 4.64810, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.64810 to 4.64249, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.64249\n",
      "\n",
      "Epoch 7: val_loss improved from 4.64249 to 4.62617, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 4.62617 to 4.61706, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61706\n",
      "\n",
      "Epoch 10: val_loss improved from 4.61706 to 4.61704, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.61704\n",
      "\n",
      "Epoch 17: val_loss improved from 4.61704 to 4.61271, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 4.61271 to 4.60169, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.60169\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.60169\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.60169\n",
      "\n",
      "Epoch 22: val_loss improved from 4.60169 to 4.59039, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 4.59039 to 4.58045, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 4.58045 to 4.57159, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 4.57159 to 4.56689, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 4.56689 to 4.55545, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 27: val_loss improved from 4.55545 to 4.55049, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 28: val_loss improved from 4.55049 to 4.53363, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 4.53363 to 4.52692, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 4.52692 to 4.51842, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 4.51842 to 4.50877, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 4.50877 to 4.49856, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 33: val_loss improved from 4.49856 to 4.49064, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 34: val_loss did not improve from 4.49064\n",
      "\n",
      "Epoch 35: val_loss improved from 4.49064 to 4.48428, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 4.48428\n",
      "\n",
      "Epoch 37: val_loss improved from 4.48428 to 4.47103, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 38: val_loss improved from 4.47103 to 4.44766, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 39: val_loss improved from 4.44766 to 4.43643, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 40: val_loss improved from 4.43643 to 4.43575, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 41: val_loss improved from 4.43575 to 4.42304, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 4.42304 to 4.41124, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.41124\n",
      "\n",
      "Epoch 44: val_loss did not improve from 4.41124\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.41124\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.41124\n",
      "\n",
      "Epoch 47: val_loss improved from 4.41124 to 4.41042, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 48: val_loss did not improve from 4.41042\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.41042\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.41042\n",
      "\n",
      "Epoch 51: val_loss improved from 4.41042 to 4.39941, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 52: val_loss improved from 4.39941 to 4.38807, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 53: val_loss improved from 4.38807 to 4.38448, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.38448\n",
      "\n",
      "Epoch 55: val_loss did not improve from 4.38448\n",
      "\n",
      "Epoch 56: val_loss improved from 4.38448 to 4.37972, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 57: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 59: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 60: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 61: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 62: val_loss did not improve from 4.37972\n",
      "\n",
      "Epoch 63: val_loss improved from 4.37972 to 4.37884, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 64: val_loss did not improve from 4.37884\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.37884\n",
      "\n",
      "Epoch 66: val_loss did not improve from 4.37884\n",
      "\n",
      "Epoch 67: val_loss improved from 4.37884 to 4.37225, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 68: val_loss did not improve from 4.37225\n",
      "\n",
      "Epoch 69: val_loss improved from 4.37225 to 4.36834, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 70: val_loss improved from 4.36834 to 4.36246, saving model to best_model_2_32_8_1e-06.keras\n",
      "\n",
      "Epoch 71: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 72: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 73: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 74: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 75: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 76: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 77: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 78: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 79: val_loss did not improve from 4.36246\n",
      "\n",
      "Epoch 80: val_loss did not improve from 4.36246\n",
      "New Best Model Found: val_loss=4.362457275390625, params=(2, 32, 8, 1e-06)\n",
      "Training with params: num_blocks=2, ff_dim=32, batch_size=8, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.62209, saving model to best_model_2_32_8_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 4.62209\n",
      "\n",
      "Epoch 3: val_loss improved from 4.62209 to 4.61392, saving model to best_model_2_32_8_1e-05.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 5: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.61392\n",
      "\n",
      "Epoch 10: val_loss improved from 4.61392 to 4.61261, saving model to best_model_2_32_8_1e-05.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.61261\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.61261\n",
      "Training with params: num_blocks=2, ff_dim=32, batch_size=16, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.96884, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.96884 to 4.84400, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.84400 to 4.76511, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.76511 to 4.70935, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.70935 to 4.68669, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.68669 to 4.67627, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.67627 to 4.66299, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 4.66299 to 4.64067, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.64067\n",
      "\n",
      "Epoch 10: val_loss improved from 4.64067 to 4.63309, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.63309\n",
      "\n",
      "Epoch 18: val_loss improved from 4.63309 to 4.63300, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 19: val_loss improved from 4.63300 to 4.62721, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 4.62721 to 4.62257, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 4.62257 to 4.61455, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.61455\n",
      "\n",
      "Epoch 23: val_loss improved from 4.61455 to 4.61284, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.61284\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.61284\n",
      "\n",
      "Epoch 26: val_loss improved from 4.61284 to 4.59444, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 27: val_loss improved from 4.59444 to 4.59321, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 28: val_loss improved from 4.59321 to 4.58756, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 4.58756 to 4.58112, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 4.58112 to 4.57904, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 4.57904 to 4.56610, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 4.56610 to 4.55527, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.55527\n",
      "\n",
      "Epoch 34: val_loss improved from 4.55527 to 4.54890, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 4.54890 to 4.54628, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 4.54628\n",
      "\n",
      "Epoch 37: val_loss improved from 4.54628 to 4.54076, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 38: val_loss improved from 4.54076 to 4.52996, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 4.52996\n",
      "\n",
      "Epoch 40: val_loss did not improve from 4.52996\n",
      "\n",
      "Epoch 41: val_loss improved from 4.52996 to 4.52490, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 4.52490 to 4.52400, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 43: val_loss improved from 4.52400 to 4.51925, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 4.51925\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.51925\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.51925\n",
      "\n",
      "Epoch 47: val_loss improved from 4.51925 to 4.50321, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 48: val_loss did not improve from 4.50321\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.50321\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.50321\n",
      "\n",
      "Epoch 51: val_loss improved from 4.50321 to 4.49068, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 52: val_loss did not improve from 4.49068\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.49068\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.49068\n",
      "\n",
      "Epoch 55: val_loss did not improve from 4.49068\n",
      "\n",
      "Epoch 56: val_loss did not improve from 4.49068\n",
      "\n",
      "Epoch 57: val_loss improved from 4.49068 to 4.48807, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.48807\n",
      "\n",
      "Epoch 59: val_loss improved from 4.48807 to 4.47192, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 60: val_loss did not improve from 4.47192\n",
      "\n",
      "Epoch 61: val_loss did not improve from 4.47192\n",
      "\n",
      "Epoch 62: val_loss did not improve from 4.47192\n",
      "\n",
      "Epoch 63: val_loss did not improve from 4.47192\n",
      "\n",
      "Epoch 64: val_loss improved from 4.47192 to 4.46946, saving model to best_model_2_32_16_1e-06.keras\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 66: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 67: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 68: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 69: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 70: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 71: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 72: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 73: val_loss did not improve from 4.46946\n",
      "\n",
      "Epoch 74: val_loss did not improve from 4.46946\n",
      "Training with params: num_blocks=2, ff_dim=32, batch_size=16, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.74800, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.74800 to 4.66693, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 4.66693\n",
      "\n",
      "Epoch 4: val_loss improved from 4.66693 to 4.65902, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.65902 to 4.61428, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.61428\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.61428\n",
      "\n",
      "Epoch 8: val_loss improved from 4.61428 to 4.56459, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.56459\n",
      "\n",
      "Epoch 10: val_loss improved from 4.56459 to 4.52648, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 4.52648 to 4.50718, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 4.50718 to 4.42749, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.42749\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.42749\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.42749\n",
      "\n",
      "Epoch 16: val_loss improved from 4.42749 to 4.42066, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.42066\n",
      "\n",
      "Epoch 26: val_loss improved from 4.42066 to 4.42036, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 31: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 34: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 35: val_loss did not improve from 4.42036\n",
      "\n",
      "Epoch 36: val_loss improved from 4.42036 to 4.38138, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 37: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 38: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 39: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 40: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 41: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 42: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.38138\n",
      "\n",
      "Epoch 44: val_loss improved from 4.38138 to 4.34743, saving model to best_model_2_32_16_1e-05.keras\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 47: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 48: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 51: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 52: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.34743\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.34743\n",
      "New Best Model Found: val_loss=4.347432613372803, params=(2, 32, 16, 1e-05)\n",
      "Training with params: num_blocks=2, ff_dim=64, batch_size=8, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.76205, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.76205 to 4.68964, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.68964 to 4.65889, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.65889 to 4.65089, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.65089 to 4.64323, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.64323\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.64323\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.64323\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.64323\n",
      "\n",
      "Epoch 10: val_loss improved from 4.64323 to 4.64158, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 4.64158 to 4.63451, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.63451\n",
      "\n",
      "Epoch 13: val_loss improved from 4.63451 to 4.62363, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.62363\n",
      "\n",
      "Epoch 15: val_loss improved from 4.62363 to 4.62052, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.62052\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.62052\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.62052\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.62052\n",
      "\n",
      "Epoch 20: val_loss improved from 4.62052 to 4.61937, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 4.61937 to 4.61931, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 4.61931 to 4.61539, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4.61539\n",
      "\n",
      "Epoch 24: val_loss improved from 4.61539 to 4.61216, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 4.61216 to 4.60504, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.60504\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.60504\n",
      "\n",
      "Epoch 28: val_loss improved from 4.60504 to 4.59505, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 4.59505 to 4.59015, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 4.59015 to 4.57308, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 4.57308 to 4.56606, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4.56606\n",
      "\n",
      "Epoch 33: val_loss did not improve from 4.56606\n",
      "\n",
      "Epoch 34: val_loss did not improve from 4.56606\n",
      "\n",
      "Epoch 35: val_loss improved from 4.56606 to 4.56195, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 36: val_loss improved from 4.56195 to 4.55229, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 37: val_loss improved from 4.55229 to 4.54343, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 38: val_loss did not improve from 4.54343\n",
      "\n",
      "Epoch 39: val_loss improved from 4.54343 to 4.53622, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 40: val_loss improved from 4.53622 to 4.51810, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 41: val_loss improved from 4.51810 to 4.50792, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 4.50792 to 4.49643, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.49643\n",
      "\n",
      "Epoch 44: val_loss improved from 4.49643 to 4.47696, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.47696\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.47696\n",
      "\n",
      "Epoch 47: val_loss did not improve from 4.47696\n",
      "\n",
      "Epoch 48: val_loss improved from 4.47696 to 4.46929, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 49: val_loss improved from 4.46929 to 4.46027, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 50: val_loss improved from 4.46027 to 4.45788, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 51: val_loss did not improve from 4.45788\n",
      "\n",
      "Epoch 52: val_loss improved from 4.45788 to 4.44464, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.44464\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.44464\n",
      "\n",
      "Epoch 55: val_loss improved from 4.44464 to 4.43862, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 56: val_loss improved from 4.43862 to 4.43538, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 57: val_loss did not improve from 4.43538\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.43538\n",
      "\n",
      "Epoch 59: val_loss did not improve from 4.43538\n",
      "\n",
      "Epoch 60: val_loss did not improve from 4.43538\n",
      "\n",
      "Epoch 61: val_loss improved from 4.43538 to 4.43136, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 62: val_loss did not improve from 4.43136\n",
      "\n",
      "Epoch 63: val_loss did not improve from 4.43136\n",
      "\n",
      "Epoch 64: val_loss improved from 4.43136 to 4.42981, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.42981\n",
      "\n",
      "Epoch 66: val_loss did not improve from 4.42981\n",
      "\n",
      "Epoch 67: val_loss improved from 4.42981 to 4.42953, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 68: val_loss improved from 4.42953 to 4.42559, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 69: val_loss improved from 4.42559 to 4.41931, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 70: val_loss did not improve from 4.41931\n",
      "\n",
      "Epoch 71: val_loss did not improve from 4.41931\n",
      "\n",
      "Epoch 72: val_loss improved from 4.41931 to 4.41924, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 73: val_loss did not improve from 4.41924\n",
      "\n",
      "Epoch 74: val_loss did not improve from 4.41924\n",
      "\n",
      "Epoch 75: val_loss did not improve from 4.41924\n",
      "\n",
      "Epoch 76: val_loss improved from 4.41924 to 4.40086, saving model to best_model_2_64_8_1e-06.keras\n",
      "\n",
      "Epoch 77: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 78: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 79: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 80: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 81: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 82: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 83: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 84: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 85: val_loss did not improve from 4.40086\n",
      "\n",
      "Epoch 86: val_loss did not improve from 4.40086\n",
      "Training with params: num_blocks=2, ff_dim=64, batch_size=8, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.81691, saving model to best_model_2_64_8_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.81691 to 4.69622, saving model to best_model_2_64_8_1e-05.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.69622 to 4.65839, saving model to best_model_2_64_8_1e-05.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 4.65839\n",
      "\n",
      "Epoch 5: val_loss did not improve from 4.65839\n",
      "\n",
      "Epoch 6: val_loss improved from 4.65839 to 4.63809, saving model to best_model_2_64_8_1e-05.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.63809\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.63809\n",
      "Training with params: num_blocks=2, ff_dim=64, batch_size=16, learning_rate=1e-06\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.85889, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.85889 to 4.77589, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 4.77589 to 4.72692, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 4.72692 to 4.69199, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 4.69199 to 4.66935, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.66935 to 4.66411, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.66411 to 4.66235, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.66235\n",
      "\n",
      "Epoch 9: val_loss improved from 4.66235 to 4.65566, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.65566\n",
      "\n",
      "Epoch 11: val_loss improved from 4.65566 to 4.65338, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 4.65338 to 4.65130, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 4.65130 to 4.65103, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.65103\n",
      "\n",
      "Epoch 15: val_loss improved from 4.65103 to 4.64784, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 4.64784 to 4.63800, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 4.63800 to 4.62991, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 4.62991 to 4.62673, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 4.62673\n",
      "\n",
      "Epoch 20: val_loss improved from 4.62673 to 4.62612, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 4.62612 to 4.62584, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 4.62584 to 4.61933, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 4.61933 to 4.61197, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 4.61197 to 4.61002, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 4.61002 to 4.60389, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 4.60389 to 4.60240, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.60240\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.60240\n",
      "\n",
      "Epoch 29: val_loss improved from 4.60240 to 4.58795, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 4.58795 to 4.58216, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 4.58216 to 4.57754, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 4.57754 to 4.56564, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 33: val_loss improved from 4.56564 to 4.55793, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 34: val_loss improved from 4.55793 to 4.55148, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 4.55148 to 4.53957, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 36: val_loss improved from 4.53957 to 4.52710, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 37: val_loss improved from 4.52710 to 4.51896, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 38: val_loss improved from 4.51896 to 4.51417, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 39: val_loss improved from 4.51417 to 4.50271, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 40: val_loss improved from 4.50271 to 4.49485, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 41: val_loss did not improve from 4.49485\n",
      "\n",
      "Epoch 42: val_loss improved from 4.49485 to 4.47667, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 43: val_loss improved from 4.47667 to 4.46485, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 44: val_loss improved from 4.46485 to 4.45598, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 45: val_loss improved from 4.45598 to 4.45017, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 46: val_loss improved from 4.45017 to 4.44593, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 47: val_loss did not improve from 4.44593\n",
      "\n",
      "Epoch 48: val_loss improved from 4.44593 to 4.44475, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 49: val_loss improved from 4.44475 to 4.43317, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.43317\n",
      "\n",
      "Epoch 51: val_loss did not improve from 4.43317\n",
      "\n",
      "Epoch 52: val_loss improved from 4.43317 to 4.41646, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.41646\n",
      "\n",
      "Epoch 54: val_loss improved from 4.41646 to 4.40603, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 55: val_loss did not improve from 4.40603\n",
      "\n",
      "Epoch 56: val_loss improved from 4.40603 to 4.40136, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 57: val_loss did not improve from 4.40136\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.40136\n",
      "\n",
      "Epoch 59: val_loss did not improve from 4.40136\n",
      "\n",
      "Epoch 60: val_loss did not improve from 4.40136\n",
      "\n",
      "Epoch 61: val_loss improved from 4.40136 to 4.38964, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 62: val_loss improved from 4.38964 to 4.38767, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 63: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 64: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 66: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 67: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 68: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 69: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 70: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 71: val_loss did not improve from 4.38767\n",
      "\n",
      "Epoch 72: val_loss improved from 4.38767 to 4.38363, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 73: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 74: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 75: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 76: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 77: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 78: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 79: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 80: val_loss did not improve from 4.38363\n",
      "\n",
      "Epoch 81: val_loss improved from 4.38363 to 4.37535, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 82: val_loss did not improve from 4.37535\n",
      "\n",
      "Epoch 83: val_loss improved from 4.37535 to 4.37168, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 84: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 85: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 86: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 87: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 88: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 89: val_loss did not improve from 4.37168\n",
      "\n",
      "Epoch 90: val_loss improved from 4.37168 to 4.36537, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 91: val_loss improved from 4.36537 to 4.35284, saving model to best_model_2_64_16_1e-06.keras\n",
      "\n",
      "Epoch 92: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 93: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 94: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 95: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 96: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 97: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 98: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 99: val_loss did not improve from 4.35284\n",
      "\n",
      "Epoch 100: val_loss did not improve from 4.35284\n",
      "Training with params: num_blocks=2, ff_dim=64, batch_size=16, learning_rate=1e-05\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.74125, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 4.74125 to 4.65832, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 4.65832\n",
      "\n",
      "Epoch 4: val_loss did not improve from 4.65832\n",
      "\n",
      "Epoch 5: val_loss improved from 4.65832 to 4.65396, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 4.65396 to 4.64765, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 4.64765 to 4.63069, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 4.63069 to 4.57203, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.57203\n",
      "\n",
      "Epoch 10: val_loss improved from 4.57203 to 4.55079, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 4.55079 to 4.55073, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.55073\n",
      "\n",
      "Epoch 13: val_loss improved from 4.55073 to 4.49972, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.49972\n",
      "\n",
      "Epoch 15: val_loss improved from 4.49972 to 4.46887, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.46887\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4.46887\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4.46887\n",
      "\n",
      "Epoch 19: val_loss improved from 4.46887 to 4.44005, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 20: val_loss did not improve from 4.44005\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4.44005\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4.44005\n",
      "\n",
      "Epoch 23: val_loss improved from 4.44005 to 4.35996, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 31: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4.35996\n",
      "\n",
      "Epoch 33: val_loss improved from 4.35996 to 4.33379, saving model to best_model_2_64_16_1e-05.keras\n",
      "\n",
      "Epoch 34: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 35: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 36: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 37: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 38: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 39: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 40: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 41: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 42: val_loss did not improve from 4.33379\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.33379\n",
      "New Best Model Found: val_loss=4.333792209625244, params=(2, 64, 16, 1e-05)\n",
      "Best model and parameters saved to: /content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/best_mod\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "best_model, best_history, best_params = grid_search(param_grid, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Save the best model and parameters\n",
    "final_best_model_path = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/best_mod'\n",
    "os.makedirs(final_best_model_path, exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(os.path.join(final_best_model_path, 'best_model_overall_phf.keras'))\n",
    "\n",
    "# Save the best hyperparameters\n",
    "import json\n",
    "best_params_path = os.path.join(final_best_model_path, 'best_model_params_phf.json')\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'num_blocks': best_params[0],\n",
    "        'ff_dim': best_params[1],\n",
    "        'batch_size': best_params[2],\n",
    "        'learning_rate': best_params[3],\n",
    "        'validation_loss': min(best_history.history['val_loss'])  # Best validation loss\n",
    "    }, f)\n",
    "\n",
    "print(f\"Best model and parameters saved to: {final_best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10608,
     "status": "ok",
     "timestamp": 1733458649517,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "DEByuVZdvNTs",
    "outputId": "427b9164-d470-432c-a268-87810b42a664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'num_blocks': 2, 'ff_dim': 64, 'batch_size': 16, 'learning_rate': 1e-05, 'validation_loss': 4.333792209625244}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 902ms/step - categorical_accuracy: 0.0092 - loss: 4.6598\n",
      "Test Loss: 4.669348239898682\n",
      "Test Accuracy: 0.01785714365541935\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your saved best model and parameters\n",
    "final_best_model_path = '/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/best_mod'\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = os.path.join(final_best_model_path, 'best_model_overall_phf.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Load the best hyperparameters (from the saved JSON file)\n",
    "best_params_path = os.path.join(final_best_model_path, 'best_model_params_phf.json')\n",
    "\n",
    "with open(best_params_path, 'r') as file:\n",
    "    best_params = json.load(file)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPR1Rk0sZL7nIzprJLs+PX9",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
