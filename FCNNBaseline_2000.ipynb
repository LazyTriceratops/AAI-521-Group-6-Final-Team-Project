{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM7yQ2oY9pEI/drHdl91LSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LazyTriceratops/AAI-521-Group-6-Final-Team-Project/blob/HernandezModelTwo/FCNNBaseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur3goNcm6Kg4",
        "outputId": "5adabfce-00e5-4e99-f701-f9dfe87c35b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mHernandezModelTwo\u001b[m\n",
            "  main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUDwryWUlvw4",
        "outputId": "b1132e1f-0410-4931-d34e-36b1a64dfd11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AAI-521-Group-6-Final-Team-Project'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 57 (delta 27), reused 37 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 647.18 KiB | 2.21 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://Anitra-Hernandez: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@github.com/LazyTriceratops/AAI-521-Group-6-Final-Team-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AAI-521-Group-6-Final-Team-Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Emdz5G9hlH",
        "outputId": "1daeceb4-0b04-4370-ee48-edd530c689cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AAI-521-Group-6-Final-Team-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout HernandezModelTwo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j9RbJsS9xYr",
        "outputId": "31b95d84-de3a-4f87-c3b3-00097b38a638"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'HernandezModelTwo' set up to track remote branch 'HernandezModelTwo' from 'origin'.\n",
            "Switched to a new branch 'HernandezModelTwo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import ParameterGrid\n"
      ],
      "metadata": {
        "id": "_KYp-v8c-X7v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iDBLlph_kAf",
        "outputId": "eecc91f3-e2d2-4fad-ca94-b0b2b07557ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/AAI-521/Final Project/Models/combined_dataset.pkl\")\n",
        "\n",
        "train_data = data[data['split'] == \"train\"]\n",
        "val_data = data[data['split'] == 'val']\n",
        "test_data = data[data['split'] == 'test']\n"
      ],
      "metadata": {
        "id": "EKC4mAcCAky5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(train_labels.numpy(), return_counts=True)\n",
        "print(\"Class Distribution:\", dict(zip(unique, counts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2hCWhueAMUI",
        "outputId": "3229560b-e840-4eb9-a9e0-2245ce1dae9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution: {0: 5, 1: 11, 2: 10, 3: 12, 4: 5, 5: 11, 6: 4, 7: 10, 8: 8, 9: 9, 10: 7, 11: 6, 12: 10, 13: 8, 14: 11, 15: 9, 16: 7, 17: 9, 18: 7, 19: 8, 20: 11, 21: 6, 22: 8, 23: 6, 24: 9, 25: 10, 26: 5, 27: 8, 28: 5, 29: 9, 30: 6, 31: 9, 32: 12, 33: 6, 34: 10, 35: 7, 36: 8, 37: 10, 38: 7, 39: 8, 40: 8, 41: 7, 42: 6, 43: 6, 44: 8, 45: 10, 46: 8, 47: 9, 48: 10, 49: 8, 50: 7, 51: 9, 52: 8, 53: 7, 54: 9, 55: 6, 56: 10, 57: 5, 58: 7, 59: 9, 60: 8, 61: 5, 62: 7, 63: 5, 64: 6, 65: 8, 66: 7, 67: 8, 68: 8, 69: 8, 70: 7, 71: 8, 72: 9, 73: 5, 74: 6, 75: 7, 76: 6, 77: 9, 78: 5, 79: 6, 80: 4, 81: 6, 82: 6, 83: 7, 84: 7, 85: 6, 86: 6, 87: 5, 88: 9, 89: 7, 90: 5, 91: 5, 92: 7, 93: 6, 94: 5, 95: 5, 96: 6, 97: 8, 98: 7, 99: 9, 100: 7, 101: 8, 102: 7, 103: 7, 104: 5, 105: 5, 106: 6, 107: 9, 108: 6, 109: 8, 110: 8, 111: 7, 112: 8, 113: 5, 114: 6, 115: 6, 116: 5, 117: 7, 118: 6, 119: 5, 120: 6, 121: 8, 122: 4, 123: 6, 124: 5, 125: 5, 126: 7, 127: 5, 128: 8, 129: 6, 130: 7, 131: 7, 132: 6, 133: 7, 134: 7, 135: 9, 136: 7, 137: 8, 138: 8, 139: 6, 140: 7, 141: 7, 142: 5, 143: 7, 144: 6, 145: 8, 146: 5, 147: 7, 148: 5, 149: 6, 150: 5, 151: 7, 152: 8, 153: 7, 154: 7, 155: 5, 156: 7, 157: 6, 158: 4, 159: 5, 160: 7, 161: 7, 162: 6, 163: 7, 164: 4, 165: 4, 166: 6, 167: 5, 168: 6, 169: 5, 170: 5, 171: 7, 172: 6, 173: 6, 174: 6, 175: 6, 176: 10, 177: 7, 178: 4, 179: 5, 180: 6, 181: 6, 182: 4, 183: 7, 184: 7, 185: 7, 186: 7, 187: 5, 188: 5, 189: 5, 190: 6, 191: 5, 192: 6, 193: 5, 194: 4, 195: 6, 196: 6, 197: 7, 198: 7, 199: 6, 200: 8, 201: 4, 202: 4, 203: 5, 204: 7, 205: 4, 206: 5, 207: 6, 208: 4, 209: 6, 210: 8, 211: 5, 212: 5, 213: 7, 214: 4, 215: 4, 216: 6, 217: 5, 218: 5, 219: 4, 220: 6, 221: 5, 222: 6, 223: 4, 224: 4, 225: 6, 226: 5, 227: 6, 228: 6, 229: 5, 230: 5, 231: 5, 232: 3, 233: 4, 234: 5, 235: 5, 236: 6, 237: 3, 238: 6, 239: 4, 240: 6, 241: 7, 242: 7, 243: 6, 244: 6, 245: 5, 246: 4, 247: 6, 248: 4, 249: 9, 250: 5, 251: 8, 252: 6, 253: 5, 254: 4, 255: 4, 256: 4, 257: 6, 258: 8, 259: 6, 260: 4, 261: 5, 262: 4, 263: 4, 264: 6, 265: 7, 266: 5, 267: 7, 268: 5, 269: 3, 270: 6, 271: 7, 272: 8, 273: 5, 274: 5, 275: 3, 276: 5, 277: 4, 278: 5, 279: 9, 280: 5, 281: 5, 282: 4, 283: 4, 284: 7, 285: 6, 286: 3, 287: 6, 288: 6, 289: 6, 290: 5, 291: 5, 292: 5, 293: 5, 294: 5, 295: 5, 296: 6, 297: 5, 298: 1, 299: 4, 300: 5, 301: 5, 302: 7, 303: 6, 304: 4, 305: 3, 306: 5, 307: 3, 308: 4, 309: 4, 310: 3, 311: 3, 312: 4, 313: 4, 314: 5, 315: 8, 316: 5, 317: 6, 318: 3, 319: 5, 320: 5, 321: 6, 322: 3, 323: 4, 324: 6, 325: 5, 326: 5, 327: 4, 328: 4, 329: 6, 330: 5, 331: 4, 332: 6, 333: 4, 334: 5, 335: 5, 336: 6, 337: 4, 338: 4, 339: 3, 340: 5, 341: 6, 342: 5, 343: 5, 344: 5, 345: 6, 346: 6, 347: 4, 348: 5, 349: 4, 350: 5, 351: 4, 352: 7, 353: 7, 354: 7, 355: 6, 356: 7, 357: 4, 358: 5, 359: 5, 360: 5, 361: 5, 362: 5, 363: 4, 364: 6, 365: 5, 366: 6, 367: 5, 368: 4, 369: 4, 370: 7, 371: 6, 372: 4, 373: 6, 374: 3, 375: 4, 376: 5, 377: 5, 378: 6, 379: 5, 380: 7, 381: 6, 382: 8, 383: 8, 384: 6, 385: 6, 386: 3, 387: 5, 388: 5, 389: 6, 390: 6, 391: 4, 392: 6, 393: 6, 394: 5, 395: 5, 396: 3, 397: 7, 398: 6, 399: 7, 400: 5, 401: 6, 402: 2, 403: 4, 404: 4, 405: 5, 406: 6, 407: 3, 408: 4, 409: 6, 410: 5, 411: 5, 412: 5, 413: 4, 414: 5, 415: 5, 416: 5, 417: 4, 418: 6, 419: 6, 420: 5, 421: 5, 422: 4, 423: 5, 424: 5, 425: 5, 426: 4, 427: 3, 428: 6, 429: 4, 430: 4, 431: 4, 432: 6, 433: 5, 434: 7, 435: 4, 436: 6, 437: 5, 438: 5, 439: 8, 440: 4, 441: 6, 442: 5, 443: 7, 444: 6, 445: 5, 446: 4, 447: 6, 448: 5, 449: 6, 450: 5, 451: 6, 452: 5, 453: 6, 454: 5, 455: 5, 456: 4, 457: 4, 458: 4, 459: 7, 460: 5, 461: 3, 462: 5, 463: 7, 464: 5, 465: 5, 466: 5, 467: 6, 468: 6, 469: 6, 470: 7, 471: 6, 472: 6, 473: 3, 474: 6, 475: 5, 476: 4, 477: 4, 478: 5, 479: 4, 480: 5, 481: 4, 482: 3, 483: 5, 484: 5, 485: 5, 486: 4, 487: 4, 488: 4, 489: 4, 490: 4, 491: 6, 492: 5, 493: 5, 494: 5, 495: 5, 496: 4, 497: 6, 498: 4, 499: 4, 500: 5, 501: 4, 502: 5, 503: 6, 504: 5, 505: 4, 506: 5, 507: 4, 508: 3, 509: 6, 510: 5, 511: 4, 512: 5, 513: 4, 514: 5, 515: 5, 516: 5, 517: 5, 518: 4, 519: 4, 520: 6, 521: 4, 522: 4, 523: 3, 524: 3, 525: 3, 526: 4, 527: 6, 528: 4, 529: 4, 530: 5, 531: 4, 532: 4, 533: 2, 534: 4, 535: 6, 536: 3, 537: 5, 538: 5, 539: 2, 540: 5, 541: 4, 542: 4, 543: 6, 544: 4, 545: 6, 546: 4, 547: 6, 548: 5, 549: 3, 550: 5, 551: 4, 552: 5, 553: 4, 554: 4, 555: 4, 556: 4, 557: 4, 558: 3, 559: 6, 560: 5, 561: 7, 562: 4, 563: 4, 564: 5, 565: 4, 566: 4, 567: 5, 568: 4, 569: 5, 570: 6, 571: 5, 572: 5, 573: 4, 574: 5, 575: 5, 576: 5, 577: 3, 578: 3, 579: 4, 580: 5, 581: 3, 582: 4, 583: 4, 584: 5, 585: 4, 586: 4, 587: 4, 588: 5, 589: 4, 590: 2, 591: 4, 592: 6, 593: 4, 594: 6, 595: 5, 596: 4, 597: 5, 598: 4, 599: 4, 600: 4, 601: 3, 602: 5, 603: 4, 604: 5, 605: 4, 606: 6, 607: 4, 608: 5, 609: 4, 610: 4, 611: 6, 612: 3, 613: 5, 614: 4, 615: 5, 616: 3, 617: 4, 618: 3, 619: 6, 620: 4, 621: 4, 622: 6, 623: 5, 624: 4, 625: 3, 626: 5, 627: 3, 628: 1, 629: 6, 630: 3, 631: 6, 632: 5, 633: 5, 634: 4, 635: 5, 636: 3, 637: 5, 638: 4, 639: 6, 640: 4, 641: 3, 642: 4, 643: 5, 644: 5, 645: 5, 646: 4, 647: 6, 648: 4, 649: 4, 650: 6, 651: 3, 652: 5, 653: 5, 654: 3, 655: 5, 656: 6, 657: 5, 658: 4, 659: 6, 660: 5, 661: 4, 662: 2, 663: 4, 664: 4, 665: 5, 666: 5, 667: 2, 668: 5, 669: 4, 670: 5, 671: 4, 672: 4, 673: 5, 674: 5, 675: 5, 676: 4, 677: 5, 678: 6, 679: 5, 680: 5, 681: 5, 682: 3, 683: 5, 684: 4, 685: 5, 686: 3, 687: 5, 688: 3, 689: 5, 690: 5, 691: 5, 692: 3, 693: 4, 694: 2, 695: 5, 696: 5, 697: 6, 698: 3, 699: 2, 700: 5, 701: 4, 702: 3, 703: 4, 704: 4, 705: 5, 706: 6, 707: 4, 708: 4, 709: 4, 710: 4, 711: 6, 712: 3, 713: 2, 714: 4, 715: 4, 716: 5, 717: 3, 718: 4, 719: 5, 720: 5, 721: 4, 722: 5, 723: 2, 724: 5, 725: 5, 726: 5, 727: 3, 728: 4, 729: 3, 730: 4, 731: 5, 732: 3, 733: 3, 734: 5, 735: 4, 736: 5, 737: 5, 738: 3, 739: 4, 740: 4, 741: 2, 742: 5, 743: 4, 744: 4, 745: 5, 746: 5, 747: 2, 748: 4, 749: 4, 750: 3, 751: 4, 752: 5, 753: 5, 754: 5, 755: 5, 756: 3, 757: 4, 758: 2, 759: 5, 760: 5, 761: 4, 762: 5, 763: 2, 764: 4, 765: 2, 766: 2, 767: 4, 768: 5, 769: 7, 770: 3, 771: 3, 772: 4, 773: 4, 774: 5, 775: 6, 776: 5, 777: 6, 778: 3, 779: 3, 780: 5, 781: 4, 782: 3, 783: 4, 784: 3, 785: 4, 786: 4, 787: 5, 788: 5, 789: 5, 790: 5, 791: 3, 792: 6, 793: 5, 794: 6, 795: 6, 796: 4, 797: 4, 798: 4, 799: 4, 800: 2, 801: 5, 802: 4, 803: 3, 804: 4, 805: 4, 806: 4, 807: 3, 808: 6, 809: 4, 810: 4, 811: 5, 812: 5, 813: 4, 814: 4, 815: 5, 816: 3, 817: 5, 818: 4, 819: 3, 820: 3, 821: 5, 822: 5, 823: 4, 824: 3, 825: 4, 826: 3, 827: 4, 828: 3, 829: 4, 830: 2, 831: 3, 832: 6, 833: 5, 834: 5, 835: 4, 836: 4, 837: 5, 838: 2, 839: 5, 840: 5, 841: 3, 842: 4, 843: 3, 844: 3, 845: 4, 846: 5, 847: 3, 848: 5, 849: 6, 850: 4, 851: 3, 852: 5, 853: 5, 854: 5, 855: 5, 856: 3, 857: 5, 858: 5, 859: 4, 860: 3, 861: 6, 862: 4, 863: 4, 864: 5, 865: 3, 866: 3, 867: 5, 868: 3, 869: 5, 870: 5, 871: 4, 872: 5, 873: 5, 874: 4, 875: 3, 876: 4, 877: 4, 878: 4, 879: 5, 880: 4, 881: 4, 882: 4, 883: 6, 884: 3, 885: 4, 886: 5, 887: 3, 888: 4, 889: 2, 890: 3, 891: 3, 892: 3, 893: 5, 894: 2, 895: 3, 896: 3, 897: 5, 898: 3, 899: 3, 900: 5, 901: 4, 902: 2, 903: 3, 904: 5, 905: 2, 906: 3, 907: 4, 908: 4, 909: 5, 910: 4, 911: 4, 912: 5, 913: 4, 914: 6, 915: 4, 916: 5, 917: 4, 918: 3, 919: 5, 920: 5, 921: 4, 922: 4, 923: 5, 924: 2, 925: 3, 926: 3, 927: 4, 928: 4, 929: 4, 930: 4, 931: 4, 932: 3, 933: 4, 934: 6, 935: 4, 936: 2, 937: 5, 938: 4, 939: 2, 940: 6, 941: 4, 942: 4, 943: 4, 944: 3, 945: 4, 946: 4, 947: 3, 948: 3, 949: 5, 950: 4, 951: 5, 952: 5, 953: 5, 954: 4, 955: 5, 956: 4, 957: 4, 958: 4, 959: 4, 960: 5, 961: 4, 962: 4, 963: 3, 964: 4, 965: 6, 966: 5, 967: 3, 968: 4, 969: 4, 970: 2, 971: 4, 972: 4, 973: 4, 974: 4, 975: 3, 976: 4, 977: 5, 978: 5, 979: 3, 980: 5, 981: 5, 982: 3, 983: 4, 984: 5, 985: 5, 986: 3, 987: 4, 988: 5, 989: 3, 990: 4, 991: 5, 992: 2, 993: 6, 994: 4, 995: 2, 996: 6, 997: 4, 998: 4, 999: 5, 1000: 5, 1001: 5, 1002: 2, 1003: 2, 1004: 5, 1005: 4, 1006: 2, 1007: 3, 1008: 5, 1009: 3, 1010: 4, 1011: 4, 1012: 3, 1013: 4, 1014: 3, 1015: 3, 1016: 4, 1017: 5, 1018: 7, 1019: 5, 1020: 5, 1021: 4, 1022: 5, 1023: 3, 1024: 5, 1025: 3, 1026: 4, 1027: 3, 1028: 5, 1029: 3, 1030: 5, 1031: 3, 1032: 3, 1033: 3, 1034: 4, 1035: 4, 1036: 6, 1037: 5, 1038: 4, 1039: 3, 1040: 4, 1041: 4, 1042: 4, 1043: 2, 1044: 2, 1045: 5, 1046: 2, 1047: 4, 1048: 3, 1049: 4, 1050: 2, 1051: 4, 1052: 4, 1053: 5, 1054: 3, 1055: 5, 1056: 4, 1057: 1, 1058: 4, 1059: 5, 1060: 3, 1061: 4, 1062: 4, 1063: 5, 1064: 5, 1065: 4, 1066: 6, 1067: 5, 1068: 3, 1069: 3, 1070: 4, 1071: 4, 1072: 4, 1073: 4, 1074: 4, 1075: 5, 1076: 4, 1077: 4, 1078: 4, 1079: 4, 1080: 4, 1081: 4, 1082: 5, 1083: 3, 1084: 4, 1085: 4, 1086: 3, 1087: 5, 1088: 3, 1089: 2, 1090: 3, 1091: 5, 1092: 4, 1093: 3, 1094: 3, 1095: 5, 1096: 6, 1097: 5, 1098: 3, 1099: 3, 1100: 6, 1101: 4, 1102: 2, 1103: 2, 1104: 4, 1105: 3, 1106: 2, 1107: 4, 1108: 4, 1109: 5, 1110: 6, 1111: 3, 1112: 4, 1113: 4, 1114: 5, 1115: 3, 1116: 2, 1117: 4, 1118: 5, 1119: 4, 1120: 2, 1121: 3, 1122: 5, 1123: 5, 1124: 5, 1125: 6, 1126: 2, 1127: 4, 1128: 4, 1129: 3, 1130: 3, 1131: 4, 1132: 3, 1133: 4, 1134: 4, 1135: 4, 1136: 4, 1137: 4, 1138: 4, 1139: 4, 1140: 4, 1141: 3, 1142: 3, 1143: 6, 1144: 3, 1145: 4, 1146: 5, 1147: 4, 1148: 3, 1149: 3, 1150: 4, 1151: 3, 1152: 5, 1153: 3, 1154: 5, 1155: 4, 1156: 4, 1157: 3, 1158: 4, 1159: 4, 1160: 4, 1161: 4, 1162: 3, 1163: 5, 1164: 3, 1165: 5, 1166: 5, 1167: 4, 1168: 3, 1169: 4, 1170: 3, 1171: 5, 1172: 5, 1173: 4, 1174: 3, 1175: 5, 1176: 3, 1177: 4, 1178: 4, 1179: 3, 1180: 4, 1181: 5, 1182: 3, 1183: 5, 1184: 4, 1185: 3, 1186: 4, 1187: 2, 1188: 5, 1189: 3, 1190: 4, 1191: 2, 1192: 4, 1193: 5, 1194: 4, 1195: 5, 1196: 5, 1197: 1, 1198: 4, 1199: 5, 1200: 2, 1201: 5, 1202: 4, 1203: 3, 1204: 5, 1205: 3, 1206: 3, 1207: 5, 1208: 3, 1209: 4, 1210: 5, 1211: 3, 1212: 4, 1213: 3, 1214: 3, 1215: 3, 1216: 4, 1217: 4, 1218: 5, 1219: 5, 1220: 4, 1221: 5, 1222: 4, 1223: 5, 1224: 4, 1225: 3, 1226: 2, 1227: 4, 1228: 3, 1229: 5, 1230: 4, 1231: 5, 1232: 4, 1233: 4, 1234: 4, 1235: 4, 1236: 3, 1237: 4, 1238: 4, 1239: 5, 1240: 4, 1241: 6, 1242: 4, 1243: 4, 1244: 4, 1245: 5, 1246: 3, 1247: 3, 1248: 3, 1249: 4, 1250: 4, 1251: 3, 1252: 4, 1253: 4, 1254: 2, 1255: 4, 1256: 3, 1257: 4, 1258: 5, 1259: 4, 1260: 5, 1261: 3, 1262: 3, 1263: 4, 1264: 2, 1265: 4, 1266: 5, 1267: 3, 1268: 3, 1269: 4, 1270: 4, 1271: 2, 1272: 4, 1273: 5, 1274: 5, 1275: 5, 1276: 3, 1277: 3, 1278: 4, 1279: 5, 1280: 2, 1281: 3, 1282: 5, 1283: 3, 1284: 3, 1285: 2, 1286: 3, 1287: 3, 1288: 4, 1289: 3, 1290: 3, 1291: 3, 1292: 3, 1293: 3, 1294: 3, 1295: 4, 1296: 3, 1297: 3, 1298: 5, 1299: 3, 1300: 4, 1301: 3, 1302: 4, 1303: 3, 1304: 4, 1305: 3, 1306: 3, 1307: 3, 1308: 2, 1309: 3, 1310: 3, 1311: 3, 1312: 3, 1313: 4, 1314: 3, 1315: 1, 1316: 3, 1317: 2, 1318: 5, 1319: 4, 1320: 2, 1321: 2, 1322: 3, 1323: 2, 1324: 3, 1325: 3, 1326: 2, 1327: 1, 1328: 3, 1329: 3, 1330: 3, 1331: 4, 1332: 5, 1333: 3, 1334: 3, 1335: 3, 1336: 3, 1337: 2, 1338: 4, 1339: 4, 1340: 4, 1341: 3, 1342: 4, 1343: 4, 1344: 5, 1345: 3, 1346: 3, 1347: 2, 1348: 4, 1349: 4, 1350: 4, 1351: 4, 1352: 3, 1353: 4, 1354: 5, 1355: 2, 1356: 4, 1357: 4, 1358: 3, 1359: 2, 1360: 3, 1361: 2, 1362: 3, 1363: 4, 1364: 4, 1365: 4, 1366: 3, 1367: 4, 1368: 3, 1369: 2, 1370: 3, 1371: 3, 1372: 4, 1373: 4, 1374: 2, 1375: 2, 1376: 5, 1377: 4, 1378: 4, 1379: 2, 1380: 3, 1381: 4, 1382: 2, 1383: 3, 1384: 3, 1385: 4, 1386: 4, 1387: 4, 1388: 4, 1389: 4, 1390: 2, 1391: 2, 1392: 4, 1393: 4, 1394: 2, 1395: 4, 1396: 3, 1397: 3, 1398: 3, 1399: 3, 1400: 2, 1401: 5, 1402: 2, 1403: 3, 1404: 3, 1405: 3, 1406: 4, 1407: 3, 1408: 4, 1409: 2, 1410: 3, 1411: 5, 1412: 3, 1413: 2, 1414: 4, 1415: 2, 1416: 3, 1417: 2, 1418: 2, 1419: 3, 1420: 4, 1421: 1, 1422: 2, 1423: 4, 1424: 4, 1425: 3, 1426: 3, 1427: 4, 1428: 3, 1429: 3, 1430: 3, 1431: 3, 1432: 2, 1433: 3, 1434: 2, 1435: 5, 1436: 3, 1437: 4, 1438: 3, 1439: 1, 1440: 3, 1441: 3, 1442: 3, 1443: 2, 1444: 4, 1445: 3, 1446: 4, 1447: 2, 1448: 3, 1449: 3, 1450: 3, 1451: 3, 1452: 3, 1453: 4, 1454: 3, 1455: 4, 1456: 3, 1457: 3, 1458: 2, 1459: 4, 1460: 5, 1461: 3, 1462: 3, 1463: 3, 1464: 3, 1465: 3, 1466: 3, 1467: 3, 1468: 3, 1469: 5, 1470: 3, 1471: 3, 1472: 3, 1473: 3, 1474: 3, 1475: 3, 1476: 2, 1477: 3, 1478: 3, 1479: 3, 1480: 3, 1481: 4, 1482: 3, 1483: 3, 1484: 3, 1485: 3, 1486: 4, 1487: 3, 1488: 5, 1489: 3, 1490: 4, 1491: 2, 1492: 3, 1493: 3, 1494: 3, 1495: 3, 1496: 3, 1497: 3, 1498: 2, 1499: 5, 1500: 3, 1501: 1, 1502: 3, 1503: 3, 1504: 3, 1505: 3, 1506: 4, 1507: 3, 1508: 3, 1509: 3, 1510: 3, 1511: 3, 1512: 4, 1513: 3, 1514: 3, 1515: 3, 1516: 2, 1517: 3, 1518: 2, 1519: 2, 1520: 3, 1521: 3, 1522: 3, 1523: 3, 1524: 2, 1525: 4, 1526: 4, 1527: 4, 1528: 3, 1529: 2, 1530: 3, 1531: 2, 1532: 4, 1533: 3, 1534: 3, 1535: 3, 1536: 3, 1537: 4, 1538: 4, 1539: 4, 1540: 3, 1541: 3, 1542: 3, 1543: 4, 1544: 3, 1545: 3, 1546: 4, 1547: 4, 1548: 4, 1549: 4, 1550: 3, 1551: 3, 1552: 4, 1553: 3, 1554: 3, 1555: 3, 1556: 3, 1557: 3, 1558: 2, 1559: 2, 1560: 2, 1561: 3, 1562: 3, 1563: 4, 1564: 3, 1565: 3, 1566: 4, 1567: 4, 1568: 3, 1569: 3, 1570: 3, 1571: 3, 1572: 3, 1573: 3, 1574: 3, 1575: 4, 1576: 3, 1577: 3, 1578: 3, 1579: 2, 1580: 3, 1581: 3, 1582: 4, 1583: 3, 1584: 3, 1585: 2, 1586: 3, 1587: 3, 1588: 3, 1589: 4, 1590: 3, 1591: 1, 1592: 4, 1593: 3, 1594: 3, 1595: 4, 1596: 3, 1597: 3, 1598: 3, 1599: 3, 1600: 4, 1601: 3, 1602: 4, 1603: 3, 1604: 2, 1605: 3, 1606: 4, 1607: 3, 1608: 3, 1609: 4, 1610: 3, 1611: 3, 1612: 3, 1613: 3, 1614: 4, 1615: 3, 1616: 4, 1617: 3, 1618: 4, 1619: 3, 1620: 3, 1621: 3, 1622: 3, 1623: 2, 1624: 3, 1625: 3, 1626: 2, 1627: 3, 1628: 2, 1629: 4, 1630: 3, 1631: 2, 1632: 4, 1633: 4, 1634: 5, 1635: 3, 1636: 3, 1637: 2, 1638: 3, 1639: 2, 1640: 4, 1641: 3, 1642: 3, 1643: 3, 1644: 3, 1645: 3, 1646: 3, 1647: 2, 1648: 3, 1649: 3, 1650: 3, 1651: 4, 1652: 4, 1653: 2, 1654: 4, 1655: 4, 1656: 2, 1657: 4, 1658: 3, 1659: 3, 1660: 4, 1661: 3, 1662: 3, 1663: 3, 1664: 2, 1665: 4, 1666: 3, 1667: 3, 1668: 2, 1669: 3, 1670: 3, 1671: 2, 1672: 4, 1673: 4, 1674: 2, 1675: 2, 1676: 3, 1677: 3, 1678: 3, 1679: 2, 1680: 3, 1681: 4, 1682: 2, 1683: 2, 1684: 4, 1685: 4, 1686: 3, 1687: 3, 1688: 2, 1689: 3, 1690: 1, 1691: 2, 1692: 3, 1693: 2, 1694: 3, 1695: 3, 1696: 4, 1697: 4, 1698: 3, 1699: 4, 1700: 3, 1701: 2, 1702: 3, 1703: 3, 1704: 3, 1705: 4, 1706: 3, 1707: 2, 1708: 2, 1709: 3, 1710: 4, 1711: 3, 1712: 3, 1713: 3, 1714: 3, 1715: 3, 1716: 3, 1717: 3, 1718: 1, 1719: 3, 1720: 3, 1721: 5, 1722: 4, 1723: 3, 1724: 2, 1725: 3, 1726: 3, 1727: 2, 1728: 3, 1729: 3, 1730: 2, 1731: 3, 1732: 4, 1733: 3, 1734: 4, 1735: 4, 1736: 4, 1737: 3, 1738: 4, 1739: 2, 1740: 3, 1741: 3, 1742: 3, 1743: 2, 1744: 2, 1745: 3, 1746: 3, 1747: 2, 1748: 2, 1749: 3, 1750: 3, 1751: 3, 1752: 3, 1753: 4, 1754: 4, 1755: 3, 1756: 3, 1757: 3, 1758: 3, 1759: 3, 1760: 3, 1761: 4, 1762: 3, 1763: 3, 1764: 3, 1765: 4, 1766: 2, 1767: 2, 1768: 3, 1769: 2, 1770: 3, 1771: 3, 1772: 4, 1773: 2, 1774: 4, 1775: 3, 1776: 4, 1777: 4, 1778: 3, 1779: 3, 1780: 4, 1781: 4, 1782: 3, 1783: 3, 1784: 3, 1785: 3, 1786: 3, 1787: 3, 1788: 2, 1789: 3, 1790: 5, 1791: 3, 1792: 3, 1793: 3, 1794: 3, 1795: 3, 1796: 3, 1797: 3, 1798: 2, 1799: 4, 1800: 3, 1801: 4, 1802: 2, 1803: 4, 1804: 3, 1805: 3, 1806: 4, 1807: 3, 1808: 2, 1809: 3, 1810: 2, 1811: 1, 1812: 3, 1813: 3, 1814: 3, 1816: 4, 1817: 3, 1818: 3, 1819: 3, 1820: 3, 1821: 2, 1822: 2, 1823: 3, 1824: 3, 1825: 2, 1826: 4, 1827: 4, 1828: 3, 1829: 3, 1830: 3, 1831: 3, 1832: 2, 1833: 3, 1834: 3, 1835: 3, 1836: 3, 1837: 4, 1838: 4, 1839: 3, 1840: 3, 1841: 3, 1842: 5, 1843: 3, 1844: 2, 1845: 4, 1846: 2, 1847: 3, 1848: 3, 1849: 3, 1850: 4, 1851: 3, 1852: 4, 1853: 3, 1854: 4, 1855: 5, 1856: 2, 1857: 3, 1858: 3, 1859: 4, 1860: 2, 1861: 4, 1862: 4, 1863: 3, 1864: 3, 1865: 3, 1866: 3, 1867: 4, 1868: 3, 1870: 4, 1871: 3, 1872: 4, 1873: 4, 1874: 3, 1875: 2, 1876: 4, 1877: 2, 1878: 2, 1879: 4, 1880: 3, 1881: 3, 1882: 3, 1883: 1, 1884: 3, 1885: 3, 1886: 4, 1887: 4, 1888: 2, 1889: 2, 1890: 3, 1891: 4, 1892: 3, 1893: 4, 1894: 3, 1895: 4, 1896: 3, 1897: 3, 1898: 3, 1899: 3, 1900: 2, 1901: 3, 1902: 5, 1903: 2, 1904: 3, 1905: 5, 1906: 4, 1907: 3, 1908: 3, 1909: 2, 1910: 2, 1911: 3, 1912: 3, 1913: 4, 1914: 4, 1915: 3, 1916: 3, 1917: 3, 1918: 4, 1919: 2, 1920: 2, 1921: 2, 1922: 5, 1923: 3, 1924: 3, 1925: 5, 1926: 3, 1927: 2, 1928: 3, 1929: 3, 1930: 3, 1931: 4, 1932: 2, 1933: 3, 1934: 1, 1935: 3, 1936: 2, 1937: 3, 1938: 3, 1939: 3, 1940: 3, 1941: 4, 1942: 3, 1943: 3, 1944: 3, 1945: 4, 1946: 3, 1947: 4, 1948: 2, 1949: 2, 1950: 3, 1951: 3, 1952: 3, 1953: 4, 1954: 4, 1955: 2, 1956: 3, 1957: 3, 1958: 4, 1959: 3, 1960: 3, 1961: 3, 1962: 2, 1963: 3, 1964: 2, 1965: 5, 1966: 2, 1967: 4, 1968: 3, 1969: 2, 1970: 4, 1971: 4, 1972: 3, 1973: 3, 1974: 3, 1975: 4, 1976: 3, 1977: 3, 1978: 3, 1979: 1, 1980: 3, 1981: 5, 1982: 2, 1983: 3, 1984: 3, 1985: 3, 1986: 3, 1987: 3, 1988: 4, 1989: 3, 1990: 4, 1991: 4, 1992: 2, 1993: 4, 1994: 3, 1995: 5, 1996: 4, 1997: 3, 1998: 3, 1999: 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract padded_keypoints and label_index for each split\n",
        "def extract_data(split_data):\n",
        "    keypoints = np.stack(split_data['padded_keypoints'].values)  # Shape: [num_samples, 90, 33, 3]\n",
        "    labels = split_data['label_index'].values                   # Shape: [num_samples]\n",
        "    return keypoints, labels\n",
        "\n",
        "train_keypoints, train_labels = extract_data(train_data)\n",
        "val_keypoints, val_labels = extract_data(val_data)\n",
        "test_keypoints, test_labels = extract_data(test_data)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_keypoints = torch.tensor(train_keypoints, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "val_keypoints = torch.tensor(val_keypoints, dtype=torch.float32)\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "test_keypoints = torch.tensor(test_keypoints, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Adjust labels to be zero-indexed\n",
        "train_labels -= train_labels.min()\n",
        "val_labels -= val_labels.min()\n",
        "test_labels -= test_labels.min()\n",
        "\n",
        "# Dynamically calculate number of classes\n",
        "num_classes = max(train_labels.max().item(), val_labels.max().item(), test_labels.max().item()) + 1\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Train labels range: {train_labels.min()} to {train_labels.max()}\")\n",
        "print(f\"Validation labels range: {val_labels.min()} to {val_labels.max()}\")\n",
        "print(f\"Test labels range: {test_labels.min()} to {test_labels.max()}\")\n",
        "\n",
        "print(f\"Train data: {train_keypoints.shape}, {train_labels.shape}\")\n",
        "print(f\"Val data: {val_keypoints.shape}, {val_labels.shape}\")\n",
        "print(f\"Test data: {test_keypoints.shape}, {test_labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptKoFqhFFZ4P",
        "outputId": "de65b2b2-74fe-4f87-ca8b-6da65dce43d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2000\n",
            "Train labels range: 0 to 1999\n",
            "Validation labels range: 0 to 1996\n",
            "Test labels range: 0 to 1997\n",
            "Train data: torch.Size([8313, 90, 33, 3]), torch.Size([8313])\n",
            "Val data: torch.Size([2253, 90, 33, 3]), torch.Size([2253])\n",
            "Test data: torch.Size([1414, 90, 33, 3]), torch.Size([1414])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define custom dataset class\n",
        "class PoseDataset(Dataset):\n",
        "    def __init__(self, keypoints, labels):\n",
        "        self.keypoints = keypoints  # Shape: [num_samples, 90, 33, 3]\n",
        "        self.labels = labels        # Shape: [num_samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        keypoint = self.keypoints[idx]  # Shape: [90, 33, 3]\n",
        "        keypoint = keypoint.view(90, -1)  # Flatten each frame: [90, 99]\n",
        "        label = self.labels[idx]\n",
        "        return keypoint, label"
      ],
      "metadata": {
        "id": "PyqTfhh5Fbny"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_dataset = PoseDataset(train_keypoints, train_labels)\n",
        "val_dataset = PoseDataset(val_keypoints, val_labels)\n",
        "test_dataset = PoseDataset(test_keypoints, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Gzepo2ucFe91"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect a batch of data\n",
        "for inputs, labels in train_loader:\n",
        "    print(f\"Input shape: {inputs.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    break  # Just to check one batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPK4WUH7FiBc",
        "outputId": "af0bc852-6968-47fb-f423-a2877a61e623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 90, 99])\n",
            "Labels shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define EarlyStopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.stop_training = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop_training = True"
      ],
      "metadata": {
        "id": "wEyg0LhTFlvW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FCNNBaseline  model\n",
        "class FCNNBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(FCNNBaseline, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)  # First fully connected layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # Regularization\n",
        "        self.fc2 = nn.Linear(512, num_classes)  # Output layer for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: [batch_size, num_frames, input_dim]\n",
        "        x = x.mean(dim=1)  # Temporal pooling across frames -> [batch_size, input_dim]\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # [batch_size, num_classes]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "niatuIZCFpJY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = correct_preds / total_preds * 100\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    accuracy = correct_preds / total_preds * 100\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "K5CaTK5ZH5ht"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter search space\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
        "    'dropout_rate': [0.3, 0.5],  # Regularization\n",
        "    'weight_decay': [1e-5, 1e-4]  # L2 Regularization\n",
        "}\n",
        "\n",
        "# Function to train and evaluate with hyperparameters\n",
        "def train_and_evaluate_model(learning_rate, dropout_rate, weight_decay, num_classes):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize the model\n",
        "    model = FCNNBaseline(input_dim=99, num_classes=num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(50):  # Adjust as needed\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return val_acc\n"
      ],
      "metadata": {
        "id": "GhbEmJyoJS_n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform hyperparameter search\n",
        "best_val_acc = 0\n",
        "best_hyperparams = {}\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"Training with parameters: {params}\")\n",
        "    val_acc = train_and_evaluate_model(\n",
        "        learning_rate=params['learning_rate'],\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        weight_decay=params['weight_decay'],\n",
        "        num_classes=num_classes,\n",
        "    )\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_hyperparams = params\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xlr-z3jXIR3",
        "outputId": "aae2c142-487e-4a67-bd05-bff5a3219384"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6097, Train Acc: 0.10%, Val Loss: 7.5657, Val Acc: 0.13%\n",
            "Epoch 2: Train Loss: 7.4228, Train Acc: 0.16%, Val Loss: 7.4746, Val Acc: 0.18%\n",
            "Epoch 3: Train Loss: 7.2158, Train Acc: 0.32%, Val Loss: 7.4740, Val Acc: 0.13%\n",
            "Epoch 4: Train Loss: 7.0792, Train Acc: 0.30%, Val Loss: 7.4988, Val Acc: 0.13%\n",
            "Epoch 5: Train Loss: 6.9651, Train Acc: 0.51%, Val Loss: 7.5421, Val Acc: 0.27%\n",
            "Epoch 6: Train Loss: 6.8621, Train Acc: 0.45%, Val Loss: 7.5850, Val Acc: 0.22%\n",
            "Epoch 7: Train Loss: 6.7713, Train Acc: 0.49%, Val Loss: 7.6468, Val Acc: 0.22%\n",
            "Epoch 8: Train Loss: 6.6834, Train Acc: 0.81%, Val Loss: 7.6925, Val Acc: 0.13%\n",
            "Epoch 9: Train Loss: 6.6066, Train Acc: 0.75%, Val Loss: 7.7364, Val Acc: 0.27%\n",
            "Epoch 10: Train Loss: 6.5347, Train Acc: 0.97%, Val Loss: 7.8048, Val Acc: 0.27%\n",
            "Epoch 11: Train Loss: 6.4629, Train Acc: 1.08%, Val Loss: 7.8822, Val Acc: 0.36%\n",
            "Epoch 12: Train Loss: 6.4032, Train Acc: 1.18%, Val Loss: 7.9183, Val Acc: 0.40%\n",
            "Epoch 13: Train Loss: 6.3498, Train Acc: 1.08%, Val Loss: 7.9633, Val Acc: 0.44%\n",
            "Epoch 14: Train Loss: 6.2931, Train Acc: 1.40%, Val Loss: 8.0192, Val Acc: 0.44%\n",
            "Epoch 15: Train Loss: 6.2453, Train Acc: 1.53%, Val Loss: 8.1212, Val Acc: 0.58%\n",
            "Epoch 16: Train Loss: 6.1831, Train Acc: 1.73%, Val Loss: 8.1454, Val Acc: 0.58%\n",
            "Epoch 17: Train Loss: 6.1305, Train Acc: 1.94%, Val Loss: 8.1987, Val Acc: 0.53%\n",
            "Epoch 18: Train Loss: 6.0738, Train Acc: 1.89%, Val Loss: 8.2892, Val Acc: 0.62%\n",
            "Epoch 19: Train Loss: 6.0301, Train Acc: 2.08%, Val Loss: 8.2997, Val Acc: 0.62%\n",
            "Epoch 20: Train Loss: 5.9882, Train Acc: 2.20%, Val Loss: 8.3666, Val Acc: 0.71%\n",
            "Epoch 21: Train Loss: 5.9513, Train Acc: 2.31%, Val Loss: 8.4417, Val Acc: 0.62%\n",
            "Epoch 22: Train Loss: 5.9023, Train Acc: 2.65%, Val Loss: 8.4846, Val Acc: 0.75%\n",
            "Epoch 23: Train Loss: 5.8625, Train Acc: 2.48%, Val Loss: 8.5300, Val Acc: 0.71%\n",
            "Epoch 24: Train Loss: 5.8071, Train Acc: 3.06%, Val Loss: 8.6206, Val Acc: 0.62%\n",
            "Epoch 25: Train Loss: 5.7669, Train Acc: 3.01%, Val Loss: 8.6509, Val Acc: 0.71%\n",
            "Epoch 26: Train Loss: 5.7375, Train Acc: 3.42%, Val Loss: 8.7256, Val Acc: 0.71%\n",
            "Epoch 27: Train Loss: 5.6807, Train Acc: 3.49%, Val Loss: 8.7639, Val Acc: 0.58%\n",
            "Epoch 28: Train Loss: 5.6664, Train Acc: 3.36%, Val Loss: 8.8261, Val Acc: 0.53%\n",
            "Epoch 29: Train Loss: 5.6065, Train Acc: 3.87%, Val Loss: 8.8921, Val Acc: 0.58%\n",
            "Epoch 30: Train Loss: 5.5941, Train Acc: 4.05%, Val Loss: 8.9402, Val Acc: 0.62%\n",
            "Epoch 31: Train Loss: 5.5486, Train Acc: 3.98%, Val Loss: 8.9425, Val Acc: 0.62%\n",
            "Epoch 32: Train Loss: 5.5097, Train Acc: 4.33%, Val Loss: 9.0417, Val Acc: 0.71%\n",
            "Epoch 33: Train Loss: 5.4819, Train Acc: 4.33%, Val Loss: 9.0958, Val Acc: 0.67%\n",
            "Epoch 34: Train Loss: 5.4449, Train Acc: 4.47%, Val Loss: 9.1036, Val Acc: 0.67%\n",
            "Epoch 35: Train Loss: 5.4108, Train Acc: 4.61%, Val Loss: 9.1363, Val Acc: 0.53%\n",
            "Epoch 36: Train Loss: 5.3821, Train Acc: 4.79%, Val Loss: 9.1802, Val Acc: 0.62%\n",
            "Epoch 37: Train Loss: 5.3548, Train Acc: 4.81%, Val Loss: 9.2435, Val Acc: 0.53%\n",
            "Epoch 38: Train Loss: 5.3309, Train Acc: 5.00%, Val Loss: 9.3075, Val Acc: 0.71%\n",
            "Epoch 39: Train Loss: 5.2891, Train Acc: 5.28%, Val Loss: 9.3319, Val Acc: 0.67%\n",
            "Epoch 40: Train Loss: 5.2575, Train Acc: 5.75%, Val Loss: 9.3495, Val Acc: 0.53%\n",
            "Epoch 41: Train Loss: 5.2397, Train Acc: 5.80%, Val Loss: 9.4114, Val Acc: 0.71%\n",
            "Epoch 42: Train Loss: 5.2154, Train Acc: 5.92%, Val Loss: 9.4450, Val Acc: 0.67%\n",
            "Epoch 43: Train Loss: 5.1858, Train Acc: 6.18%, Val Loss: 9.4914, Val Acc: 0.67%\n",
            "Epoch 44: Train Loss: 5.1599, Train Acc: 6.48%, Val Loss: 9.4790, Val Acc: 0.71%\n",
            "Epoch 45: Train Loss: 5.1428, Train Acc: 6.09%, Val Loss: 9.5211, Val Acc: 0.71%\n",
            "Epoch 46: Train Loss: 5.1021, Train Acc: 6.70%, Val Loss: 9.5738, Val Acc: 0.53%\n",
            "Epoch 47: Train Loss: 5.0827, Train Acc: 7.04%, Val Loss: 9.6365, Val Acc: 0.75%\n",
            "Epoch 48: Train Loss: 5.0627, Train Acc: 6.89%, Val Loss: 9.6721, Val Acc: 0.53%\n",
            "Epoch 49: Train Loss: 5.0448, Train Acc: 6.92%, Val Loss: 9.6826, Val Acc: 0.62%\n",
            "Epoch 50: Train Loss: 5.0163, Train Acc: 7.10%, Val Loss: 9.7162, Val Acc: 0.58%\n",
            "Validation Accuracy: 0.5770\n",
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6113, Train Acc: 0.06%, Val Loss: 7.5763, Val Acc: 0.22%\n",
            "Epoch 2: Train Loss: 7.4742, Train Acc: 0.19%, Val Loss: 7.4931, Val Acc: 0.13%\n",
            "Epoch 3: Train Loss: 7.2898, Train Acc: 0.26%, Val Loss: 7.4514, Val Acc: 0.13%\n",
            "Epoch 4: Train Loss: 7.1617, Train Acc: 0.30%, Val Loss: 7.4449, Val Acc: 0.22%\n",
            "Epoch 5: Train Loss: 7.0613, Train Acc: 0.28%, Val Loss: 7.4367, Val Acc: 0.27%\n",
            "Epoch 6: Train Loss: 6.9775, Train Acc: 0.32%, Val Loss: 7.4453, Val Acc: 0.22%\n",
            "Epoch 7: Train Loss: 6.9093, Train Acc: 0.41%, Val Loss: 7.4519, Val Acc: 0.36%\n",
            "Epoch 8: Train Loss: 6.8337, Train Acc: 0.60%, Val Loss: 7.4653, Val Acc: 0.27%\n",
            "Epoch 9: Train Loss: 6.7739, Train Acc: 0.55%, Val Loss: 7.4914, Val Acc: 0.36%\n",
            "Epoch 10: Train Loss: 6.7164, Train Acc: 0.73%, Val Loss: 7.5113, Val Acc: 0.31%\n",
            "Epoch 11: Train Loss: 6.6544, Train Acc: 0.85%, Val Loss: 7.5388, Val Acc: 0.31%\n",
            "Epoch 12: Train Loss: 6.6105, Train Acc: 0.85%, Val Loss: 7.5770, Val Acc: 0.27%\n",
            "Epoch 13: Train Loss: 6.5623, Train Acc: 0.97%, Val Loss: 7.5804, Val Acc: 0.27%\n",
            "Epoch 14: Train Loss: 6.5065, Train Acc: 1.01%, Val Loss: 7.5983, Val Acc: 0.31%\n",
            "Epoch 15: Train Loss: 6.4722, Train Acc: 1.20%, Val Loss: 7.6362, Val Acc: 0.18%\n",
            "Epoch 16: Train Loss: 6.4286, Train Acc: 1.03%, Val Loss: 7.6572, Val Acc: 0.22%\n",
            "Epoch 17: Train Loss: 6.3893, Train Acc: 1.35%, Val Loss: 7.6865, Val Acc: 0.18%\n",
            "Epoch 18: Train Loss: 6.3482, Train Acc: 1.42%, Val Loss: 7.7041, Val Acc: 0.27%\n",
            "Epoch 19: Train Loss: 6.3103, Train Acc: 1.37%, Val Loss: 7.7343, Val Acc: 0.27%\n",
            "Epoch 20: Train Loss: 6.2726, Train Acc: 1.41%, Val Loss: 7.7498, Val Acc: 0.44%\n",
            "Epoch 21: Train Loss: 6.2283, Train Acc: 1.82%, Val Loss: 7.7818, Val Acc: 0.36%\n",
            "Epoch 22: Train Loss: 6.1947, Train Acc: 1.90%, Val Loss: 7.8095, Val Acc: 0.44%\n",
            "Epoch 23: Train Loss: 6.1645, Train Acc: 1.86%, Val Loss: 7.8145, Val Acc: 0.36%\n",
            "Epoch 24: Train Loss: 6.1243, Train Acc: 2.21%, Val Loss: 7.8527, Val Acc: 0.49%\n",
            "Epoch 25: Train Loss: 6.1090, Train Acc: 1.91%, Val Loss: 7.8510, Val Acc: 0.49%\n",
            "Epoch 26: Train Loss: 6.0676, Train Acc: 1.98%, Val Loss: 7.8763, Val Acc: 0.27%\n",
            "Epoch 27: Train Loss: 6.0465, Train Acc: 2.11%, Val Loss: 7.8950, Val Acc: 0.44%\n",
            "Epoch 28: Train Loss: 6.0256, Train Acc: 2.24%, Val Loss: 7.9070, Val Acc: 0.40%\n",
            "Epoch 29: Train Loss: 5.9946, Train Acc: 2.38%, Val Loss: 7.9153, Val Acc: 0.44%\n",
            "Epoch 30: Train Loss: 5.9663, Train Acc: 2.43%, Val Loss: 7.9482, Val Acc: 0.44%\n",
            "Epoch 31: Train Loss: 5.9354, Train Acc: 2.61%, Val Loss: 7.9711, Val Acc: 0.53%\n",
            "Epoch 32: Train Loss: 5.9042, Train Acc: 2.56%, Val Loss: 7.9859, Val Acc: 0.62%\n",
            "Epoch 33: Train Loss: 5.8899, Train Acc: 2.89%, Val Loss: 8.0041, Val Acc: 0.62%\n",
            "Epoch 34: Train Loss: 5.8616, Train Acc: 2.90%, Val Loss: 8.0177, Val Acc: 0.62%\n",
            "Epoch 35: Train Loss: 5.8331, Train Acc: 2.91%, Val Loss: 8.0199, Val Acc: 0.58%\n",
            "Epoch 36: Train Loss: 5.8299, Train Acc: 2.88%, Val Loss: 8.0513, Val Acc: 0.67%\n",
            "Epoch 37: Train Loss: 5.7994, Train Acc: 2.92%, Val Loss: 8.0509, Val Acc: 0.67%\n",
            "Epoch 38: Train Loss: 5.7663, Train Acc: 3.24%, Val Loss: 8.0792, Val Acc: 0.67%\n",
            "Epoch 39: Train Loss: 5.7558, Train Acc: 3.24%, Val Loss: 8.0924, Val Acc: 0.75%\n",
            "Epoch 40: Train Loss: 5.7365, Train Acc: 3.39%, Val Loss: 8.1087, Val Acc: 0.71%\n",
            "Epoch 41: Train Loss: 5.7135, Train Acc: 3.27%, Val Loss: 8.1162, Val Acc: 0.71%\n",
            "Epoch 42: Train Loss: 5.7033, Train Acc: 3.39%, Val Loss: 8.1225, Val Acc: 0.62%\n",
            "Epoch 43: Train Loss: 5.6683, Train Acc: 3.56%, Val Loss: 8.1331, Val Acc: 0.75%\n",
            "Epoch 44: Train Loss: 5.6583, Train Acc: 3.71%, Val Loss: 8.1453, Val Acc: 0.53%\n",
            "Epoch 45: Train Loss: 5.6505, Train Acc: 3.74%, Val Loss: 8.1457, Val Acc: 0.62%\n",
            "Epoch 46: Train Loss: 5.6286, Train Acc: 3.71%, Val Loss: 8.1832, Val Acc: 0.75%\n",
            "Epoch 47: Train Loss: 5.6080, Train Acc: 3.79%, Val Loss: 8.1854, Val Acc: 0.49%\n",
            "Epoch 48: Train Loss: 5.5970, Train Acc: 3.98%, Val Loss: 8.2031, Val Acc: 0.58%\n",
            "Epoch 49: Train Loss: 5.5642, Train Acc: 3.96%, Val Loss: 8.2179, Val Acc: 0.62%\n",
            "Epoch 50: Train Loss: 5.5657, Train Acc: 4.11%, Val Loss: 8.2215, Val Acc: 0.80%\n",
            "Validation Accuracy: 0.7989\n",
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.0005, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6127, Train Acc: 0.06%, Val Loss: 7.5793, Val Acc: 0.09%\n",
            "Epoch 2: Train Loss: 7.4514, Train Acc: 0.16%, Val Loss: 7.5074, Val Acc: 0.00%\n",
            "Epoch 3: Train Loss: 7.2282, Train Acc: 0.38%, Val Loss: 7.4970, Val Acc: 0.22%\n",
            "Epoch 4: Train Loss: 7.0733, Train Acc: 0.43%, Val Loss: 7.5258, Val Acc: 0.09%\n",
            "Epoch 5: Train Loss: 6.9465, Train Acc: 0.43%, Val Loss: 7.5404, Val Acc: 0.18%\n",
            "Epoch 6: Train Loss: 6.8411, Train Acc: 0.52%, Val Loss: 7.5744, Val Acc: 0.18%\n",
            "Epoch 7: Train Loss: 6.7447, Train Acc: 0.70%, Val Loss: 7.6402, Val Acc: 0.27%\n",
            "Epoch 8: Train Loss: 6.6609, Train Acc: 0.87%, Val Loss: 7.6591, Val Acc: 0.27%\n",
            "Epoch 9: Train Loss: 6.5830, Train Acc: 0.88%, Val Loss: 7.7169, Val Acc: 0.27%\n",
            "Epoch 10: Train Loss: 6.5095, Train Acc: 1.05%, Val Loss: 7.7652, Val Acc: 0.22%\n",
            "Epoch 11: Train Loss: 6.4438, Train Acc: 1.35%, Val Loss: 7.8230, Val Acc: 0.31%\n",
            "Epoch 12: Train Loss: 6.3841, Train Acc: 1.46%, Val Loss: 7.8929, Val Acc: 0.31%\n",
            "Epoch 13: Train Loss: 6.3336, Train Acc: 1.58%, Val Loss: 7.9466, Val Acc: 0.44%\n",
            "Epoch 14: Train Loss: 6.2779, Train Acc: 1.62%, Val Loss: 8.0164, Val Acc: 0.44%\n",
            "Epoch 15: Train Loss: 6.2261, Train Acc: 1.66%, Val Loss: 8.0751, Val Acc: 0.40%\n",
            "Epoch 16: Train Loss: 6.1786, Train Acc: 1.86%, Val Loss: 8.1245, Val Acc: 0.36%\n",
            "Epoch 17: Train Loss: 6.1333, Train Acc: 2.17%, Val Loss: 8.1830, Val Acc: 0.44%\n",
            "Epoch 18: Train Loss: 6.0859, Train Acc: 2.32%, Val Loss: 8.2287, Val Acc: 0.58%\n",
            "Epoch 19: Train Loss: 6.0494, Train Acc: 2.44%, Val Loss: 8.3196, Val Acc: 0.49%\n",
            "Epoch 20: Train Loss: 6.0027, Train Acc: 2.49%, Val Loss: 8.3421, Val Acc: 0.62%\n",
            "Epoch 21: Train Loss: 5.9687, Train Acc: 2.68%, Val Loss: 8.4108, Val Acc: 0.49%\n",
            "Epoch 22: Train Loss: 5.9281, Train Acc: 2.86%, Val Loss: 8.4595, Val Acc: 0.53%\n",
            "Epoch 23: Train Loss: 5.8881, Train Acc: 2.81%, Val Loss: 8.5062, Val Acc: 0.62%\n",
            "Epoch 24: Train Loss: 5.8431, Train Acc: 3.27%, Val Loss: 8.5750, Val Acc: 0.58%\n",
            "Epoch 25: Train Loss: 5.8083, Train Acc: 3.39%, Val Loss: 8.6258, Val Acc: 0.67%\n",
            "Epoch 26: Train Loss: 5.7802, Train Acc: 3.52%, Val Loss: 8.6884, Val Acc: 0.67%\n",
            "Epoch 27: Train Loss: 5.7350, Train Acc: 3.85%, Val Loss: 8.7442, Val Acc: 0.49%\n",
            "Epoch 28: Train Loss: 5.7005, Train Acc: 4.07%, Val Loss: 8.7833, Val Acc: 0.58%\n",
            "Epoch 29: Train Loss: 5.6705, Train Acc: 3.90%, Val Loss: 8.8354, Val Acc: 0.53%\n",
            "Epoch 30: Train Loss: 5.6368, Train Acc: 4.20%, Val Loss: 8.8878, Val Acc: 0.49%\n",
            "Epoch 31: Train Loss: 5.5987, Train Acc: 4.21%, Val Loss: 8.9194, Val Acc: 0.62%\n",
            "Epoch 32: Train Loss: 5.5691, Train Acc: 4.40%, Val Loss: 8.9750, Val Acc: 0.40%\n",
            "Epoch 33: Train Loss: 5.5368, Train Acc: 4.38%, Val Loss: 9.0584, Val Acc: 0.44%\n",
            "Epoch 34: Train Loss: 5.5030, Train Acc: 4.97%, Val Loss: 9.0815, Val Acc: 0.62%\n",
            "Epoch 35: Train Loss: 5.4677, Train Acc: 4.94%, Val Loss: 9.1431, Val Acc: 0.62%\n",
            "Epoch 36: Train Loss: 5.4407, Train Acc: 4.73%, Val Loss: 9.1727, Val Acc: 0.49%\n",
            "Epoch 37: Train Loss: 5.4122, Train Acc: 5.44%, Val Loss: 9.2084, Val Acc: 0.58%\n",
            "Epoch 38: Train Loss: 5.3687, Train Acc: 6.01%, Val Loss: 9.2474, Val Acc: 0.67%\n",
            "Epoch 39: Train Loss: 5.3428, Train Acc: 5.83%, Val Loss: 9.3096, Val Acc: 0.62%\n",
            "Epoch 40: Train Loss: 5.3187, Train Acc: 5.79%, Val Loss: 9.3632, Val Acc: 0.53%\n",
            "Epoch 41: Train Loss: 5.2997, Train Acc: 5.70%, Val Loss: 9.4127, Val Acc: 0.58%\n",
            "Epoch 42: Train Loss: 5.2618, Train Acc: 6.23%, Val Loss: 9.4273, Val Acc: 0.53%\n",
            "Epoch 43: Train Loss: 5.2317, Train Acc: 6.28%, Val Loss: 9.4861, Val Acc: 0.53%\n",
            "Epoch 44: Train Loss: 5.2094, Train Acc: 6.45%, Val Loss: 9.5292, Val Acc: 0.67%\n",
            "Epoch 45: Train Loss: 5.1815, Train Acc: 6.65%, Val Loss: 9.5533, Val Acc: 0.58%\n",
            "Epoch 46: Train Loss: 5.1529, Train Acc: 6.90%, Val Loss: 9.6114, Val Acc: 0.58%\n",
            "Epoch 47: Train Loss: 5.1249, Train Acc: 7.35%, Val Loss: 9.6517, Val Acc: 0.58%\n",
            "Epoch 48: Train Loss: 5.0925, Train Acc: 7.45%, Val Loss: 9.6842, Val Acc: 0.67%\n",
            "Epoch 49: Train Loss: 5.0726, Train Acc: 7.65%, Val Loss: 9.7243, Val Acc: 0.71%\n",
            "Epoch 50: Train Loss: 5.0582, Train Acc: 7.57%, Val Loss: 9.7744, Val Acc: 0.71%\n",
            "Validation Accuracy: 0.7102\n",
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.0005, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6139, Train Acc: 0.07%, Val Loss: 7.5880, Val Acc: 0.04%\n",
            "Epoch 2: Train Loss: 7.4817, Train Acc: 0.20%, Val Loss: 7.5106, Val Acc: 0.04%\n",
            "Epoch 3: Train Loss: 7.2661, Train Acc: 0.30%, Val Loss: 7.4819, Val Acc: 0.18%\n",
            "Epoch 4: Train Loss: 7.1219, Train Acc: 0.37%, Val Loss: 7.4705, Val Acc: 0.22%\n",
            "Epoch 5: Train Loss: 7.0211, Train Acc: 0.43%, Val Loss: 7.4684, Val Acc: 0.13%\n",
            "Epoch 6: Train Loss: 6.9282, Train Acc: 0.64%, Val Loss: 7.4718, Val Acc: 0.22%\n",
            "Epoch 7: Train Loss: 6.8530, Train Acc: 0.67%, Val Loss: 7.4835, Val Acc: 0.31%\n",
            "Epoch 8: Train Loss: 6.7808, Train Acc: 0.72%, Val Loss: 7.5010, Val Acc: 0.27%\n",
            "Epoch 9: Train Loss: 6.7198, Train Acc: 0.77%, Val Loss: 7.5221, Val Acc: 0.18%\n",
            "Epoch 10: Train Loss: 6.6606, Train Acc: 0.96%, Val Loss: 7.5377, Val Acc: 0.31%\n",
            "Epoch 11: Train Loss: 6.6088, Train Acc: 1.00%, Val Loss: 7.5607, Val Acc: 0.31%\n",
            "Epoch 12: Train Loss: 6.5547, Train Acc: 1.21%, Val Loss: 7.6006, Val Acc: 0.27%\n",
            "Epoch 13: Train Loss: 6.5063, Train Acc: 1.13%, Val Loss: 7.6257, Val Acc: 0.31%\n",
            "Epoch 14: Train Loss: 6.4596, Train Acc: 1.24%, Val Loss: 7.6553, Val Acc: 0.22%\n",
            "Epoch 15: Train Loss: 6.4147, Train Acc: 1.30%, Val Loss: 7.6808, Val Acc: 0.31%\n",
            "Epoch 16: Train Loss: 6.3826, Train Acc: 1.54%, Val Loss: 7.7093, Val Acc: 0.36%\n",
            "Epoch 17: Train Loss: 6.3421, Train Acc: 1.56%, Val Loss: 7.7384, Val Acc: 0.44%\n",
            "Epoch 18: Train Loss: 6.2988, Train Acc: 1.59%, Val Loss: 7.7568, Val Acc: 0.36%\n",
            "Epoch 19: Train Loss: 6.2692, Train Acc: 1.79%, Val Loss: 7.7936, Val Acc: 0.44%\n",
            "Epoch 20: Train Loss: 6.2310, Train Acc: 1.83%, Val Loss: 7.8160, Val Acc: 0.31%\n",
            "Epoch 21: Train Loss: 6.1905, Train Acc: 1.91%, Val Loss: 7.8361, Val Acc: 0.40%\n",
            "Epoch 22: Train Loss: 6.1618, Train Acc: 2.12%, Val Loss: 7.8634, Val Acc: 0.40%\n",
            "Epoch 23: Train Loss: 6.1295, Train Acc: 2.08%, Val Loss: 7.9038, Val Acc: 0.40%\n",
            "Epoch 24: Train Loss: 6.1002, Train Acc: 2.20%, Val Loss: 7.9057, Val Acc: 0.40%\n",
            "Epoch 25: Train Loss: 6.0688, Train Acc: 2.04%, Val Loss: 7.9410, Val Acc: 0.53%\n",
            "Epoch 26: Train Loss: 6.0345, Train Acc: 2.55%, Val Loss: 7.9469, Val Acc: 0.62%\n",
            "Epoch 27: Train Loss: 6.0067, Train Acc: 2.63%, Val Loss: 7.9819, Val Acc: 0.53%\n",
            "Epoch 28: Train Loss: 5.9845, Train Acc: 2.55%, Val Loss: 8.0066, Val Acc: 0.53%\n",
            "Epoch 29: Train Loss: 5.9576, Train Acc: 2.68%, Val Loss: 8.0225, Val Acc: 0.58%\n",
            "Epoch 30: Train Loss: 5.9293, Train Acc: 2.77%, Val Loss: 8.0411, Val Acc: 0.44%\n",
            "Epoch 31: Train Loss: 5.9059, Train Acc: 2.84%, Val Loss: 8.0575, Val Acc: 0.53%\n",
            "Epoch 32: Train Loss: 5.8791, Train Acc: 3.09%, Val Loss: 8.0806, Val Acc: 0.62%\n",
            "Epoch 33: Train Loss: 5.8495, Train Acc: 3.21%, Val Loss: 8.0913, Val Acc: 0.58%\n",
            "Epoch 34: Train Loss: 5.8211, Train Acc: 3.54%, Val Loss: 8.1085, Val Acc: 0.62%\n",
            "Epoch 35: Train Loss: 5.8001, Train Acc: 3.52%, Val Loss: 8.1420, Val Acc: 0.53%\n",
            "Epoch 36: Train Loss: 5.7700, Train Acc: 3.52%, Val Loss: 8.1509, Val Acc: 0.53%\n",
            "Epoch 37: Train Loss: 5.7440, Train Acc: 3.71%, Val Loss: 8.1699, Val Acc: 0.53%\n",
            "Epoch 38: Train Loss: 5.7281, Train Acc: 3.80%, Val Loss: 8.1813, Val Acc: 0.75%\n",
            "Epoch 39: Train Loss: 5.7064, Train Acc: 3.96%, Val Loss: 8.2083, Val Acc: 0.53%\n",
            "Epoch 40: Train Loss: 5.6882, Train Acc: 3.75%, Val Loss: 8.2138, Val Acc: 0.58%\n",
            "Epoch 41: Train Loss: 5.6620, Train Acc: 3.91%, Val Loss: 8.2262, Val Acc: 0.62%\n",
            "Epoch 42: Train Loss: 5.6416, Train Acc: 4.38%, Val Loss: 8.2449, Val Acc: 0.58%\n",
            "Epoch 43: Train Loss: 5.6219, Train Acc: 4.37%, Val Loss: 8.2562, Val Acc: 0.75%\n",
            "Epoch 44: Train Loss: 5.6030, Train Acc: 4.29%, Val Loss: 8.2854, Val Acc: 0.75%\n",
            "Epoch 45: Train Loss: 5.5765, Train Acc: 4.45%, Val Loss: 8.2883, Val Acc: 0.62%\n",
            "Epoch 46: Train Loss: 5.5557, Train Acc: 4.58%, Val Loss: 8.3066, Val Acc: 0.62%\n",
            "Epoch 47: Train Loss: 5.5290, Train Acc: 4.58%, Val Loss: 8.3104, Val Acc: 0.49%\n",
            "Epoch 48: Train Loss: 5.5323, Train Acc: 4.62%, Val Loss: 8.3318, Val Acc: 0.75%\n",
            "Epoch 49: Train Loss: 5.5039, Train Acc: 5.21%, Val Loss: 8.3417, Val Acc: 0.80%\n",
            "Epoch 50: Train Loss: 5.4892, Train Acc: 4.85%, Val Loss: 8.3495, Val Acc: 0.75%\n",
            "Validation Accuracy: 0.7545\n",
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6237, Train Acc: 0.08%, Val Loss: 7.6008, Val Acc: 0.18%\n",
            "Epoch 2: Train Loss: 7.5494, Train Acc: 0.20%, Val Loss: 7.5906, Val Acc: 0.09%\n",
            "Epoch 3: Train Loss: 7.4664, Train Acc: 0.19%, Val Loss: 7.5943, Val Acc: 0.09%\n",
            "Epoch 4: Train Loss: 7.3952, Train Acc: 0.22%, Val Loss: 7.5986, Val Acc: 0.09%\n",
            "Epoch 5: Train Loss: 7.3350, Train Acc: 0.31%, Val Loss: 7.6022, Val Acc: 0.13%\n",
            "Epoch 6: Train Loss: 7.2826, Train Acc: 0.35%, Val Loss: 7.5987, Val Acc: 0.13%\n",
            "Epoch 7: Train Loss: 7.2307, Train Acc: 0.40%, Val Loss: 7.5950, Val Acc: 0.13%\n",
            "Epoch 8: Train Loss: 7.1830, Train Acc: 0.51%, Val Loss: 7.5941, Val Acc: 0.22%\n",
            "Epoch 9: Train Loss: 7.1417, Train Acc: 0.55%, Val Loss: 7.5916, Val Acc: 0.27%\n",
            "Epoch 10: Train Loss: 7.0996, Train Acc: 0.46%, Val Loss: 7.5913, Val Acc: 0.31%\n",
            "Epoch 11: Train Loss: 7.0656, Train Acc: 0.59%, Val Loss: 7.5919, Val Acc: 0.31%\n",
            "Epoch 12: Train Loss: 7.0272, Train Acc: 0.58%, Val Loss: 7.5945, Val Acc: 0.22%\n",
            "Epoch 13: Train Loss: 6.9967, Train Acc: 0.53%, Val Loss: 7.5951, Val Acc: 0.22%\n",
            "Epoch 14: Train Loss: 6.9668, Train Acc: 0.54%, Val Loss: 7.5982, Val Acc: 0.27%\n",
            "Epoch 15: Train Loss: 6.9366, Train Acc: 0.66%, Val Loss: 7.6020, Val Acc: 0.27%\n",
            "Epoch 16: Train Loss: 6.9085, Train Acc: 0.66%, Val Loss: 7.6086, Val Acc: 0.27%\n",
            "Epoch 17: Train Loss: 6.8806, Train Acc: 0.75%, Val Loss: 7.6214, Val Acc: 0.31%\n",
            "Epoch 18: Train Loss: 6.8536, Train Acc: 0.79%, Val Loss: 7.6233, Val Acc: 0.27%\n",
            "Epoch 19: Train Loss: 6.8305, Train Acc: 0.75%, Val Loss: 7.6316, Val Acc: 0.27%\n",
            "Epoch 20: Train Loss: 6.8037, Train Acc: 0.83%, Val Loss: 7.6430, Val Acc: 0.27%\n",
            "Epoch 21: Train Loss: 6.7823, Train Acc: 0.84%, Val Loss: 7.6546, Val Acc: 0.27%\n",
            "Epoch 22: Train Loss: 6.7578, Train Acc: 0.88%, Val Loss: 7.6623, Val Acc: 0.31%\n",
            "Epoch 23: Train Loss: 6.7339, Train Acc: 1.05%, Val Loss: 7.6753, Val Acc: 0.31%\n",
            "Epoch 24: Train Loss: 6.7124, Train Acc: 0.96%, Val Loss: 7.6862, Val Acc: 0.31%\n",
            "Epoch 25: Train Loss: 6.6935, Train Acc: 1.14%, Val Loss: 7.7022, Val Acc: 0.31%\n",
            "Epoch 26: Train Loss: 6.6716, Train Acc: 1.02%, Val Loss: 7.7155, Val Acc: 0.31%\n",
            "Epoch 27: Train Loss: 6.6522, Train Acc: 1.20%, Val Loss: 7.7315, Val Acc: 0.27%\n",
            "Epoch 28: Train Loss: 6.6379, Train Acc: 1.06%, Val Loss: 7.7441, Val Acc: 0.22%\n",
            "Epoch 29: Train Loss: 6.6131, Train Acc: 1.24%, Val Loss: 7.7552, Val Acc: 0.27%\n",
            "Epoch 30: Train Loss: 6.5996, Train Acc: 1.47%, Val Loss: 7.7751, Val Acc: 0.22%\n",
            "Epoch 31: Train Loss: 6.5834, Train Acc: 1.43%, Val Loss: 7.7884, Val Acc: 0.31%\n",
            "Epoch 32: Train Loss: 6.5646, Train Acc: 1.30%, Val Loss: 7.8056, Val Acc: 0.27%\n",
            "Epoch 33: Train Loss: 6.5461, Train Acc: 1.43%, Val Loss: 7.8268, Val Acc: 0.27%\n",
            "Epoch 34: Train Loss: 6.5313, Train Acc: 1.41%, Val Loss: 7.8445, Val Acc: 0.27%\n",
            "Epoch 35: Train Loss: 6.5161, Train Acc: 1.43%, Val Loss: 7.8577, Val Acc: 0.31%\n",
            "Epoch 36: Train Loss: 6.5069, Train Acc: 1.30%, Val Loss: 7.8772, Val Acc: 0.31%\n",
            "Epoch 37: Train Loss: 6.4843, Train Acc: 1.65%, Val Loss: 7.8916, Val Acc: 0.31%\n",
            "Epoch 38: Train Loss: 6.4762, Train Acc: 1.61%, Val Loss: 7.9085, Val Acc: 0.31%\n",
            "Epoch 39: Train Loss: 6.4592, Train Acc: 1.72%, Val Loss: 7.9258, Val Acc: 0.31%\n",
            "Epoch 40: Train Loss: 6.4500, Train Acc: 1.55%, Val Loss: 7.9445, Val Acc: 0.40%\n",
            "Epoch 41: Train Loss: 6.4356, Train Acc: 1.70%, Val Loss: 7.9606, Val Acc: 0.36%\n",
            "Epoch 42: Train Loss: 6.4255, Train Acc: 1.74%, Val Loss: 7.9787, Val Acc: 0.36%\n",
            "Epoch 43: Train Loss: 6.4091, Train Acc: 1.88%, Val Loss: 7.9917, Val Acc: 0.36%\n",
            "Epoch 44: Train Loss: 6.4019, Train Acc: 1.85%, Val Loss: 8.0108, Val Acc: 0.36%\n",
            "Epoch 45: Train Loss: 6.3882, Train Acc: 2.03%, Val Loss: 8.0323, Val Acc: 0.44%\n",
            "Epoch 46: Train Loss: 6.3746, Train Acc: 2.06%, Val Loss: 8.0466, Val Acc: 0.44%\n",
            "Epoch 47: Train Loss: 6.3614, Train Acc: 2.11%, Val Loss: 8.0694, Val Acc: 0.40%\n",
            "Epoch 48: Train Loss: 6.3503, Train Acc: 2.19%, Val Loss: 8.0835, Val Acc: 0.36%\n",
            "Epoch 49: Train Loss: 6.3452, Train Acc: 2.11%, Val Loss: 8.0980, Val Acc: 0.40%\n",
            "Epoch 50: Train Loss: 6.3289, Train Acc: 2.09%, Val Loss: 8.1169, Val Acc: 0.40%\n",
            "Validation Accuracy: 0.3995\n",
            "Training with parameters: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6220, Train Acc: 0.02%, Val Loss: 7.6033, Val Acc: 0.00%\n",
            "Epoch 2: Train Loss: 7.5578, Train Acc: 0.14%, Val Loss: 7.5966, Val Acc: 0.04%\n",
            "Epoch 3: Train Loss: 7.4832, Train Acc: 0.20%, Val Loss: 7.6011, Val Acc: 0.04%\n",
            "Epoch 4: Train Loss: 7.4114, Train Acc: 0.26%, Val Loss: 7.6065, Val Acc: 0.04%\n",
            "Epoch 5: Train Loss: 7.3536, Train Acc: 0.35%, Val Loss: 7.6035, Val Acc: 0.13%\n",
            "Epoch 6: Train Loss: 7.3020, Train Acc: 0.37%, Val Loss: 7.5990, Val Acc: 0.13%\n",
            "Epoch 7: Train Loss: 7.2541, Train Acc: 0.34%, Val Loss: 7.5910, Val Acc: 0.13%\n",
            "Epoch 8: Train Loss: 7.2119, Train Acc: 0.40%, Val Loss: 7.5866, Val Acc: 0.22%\n",
            "Epoch 9: Train Loss: 7.1705, Train Acc: 0.43%, Val Loss: 7.5803, Val Acc: 0.27%\n",
            "Epoch 10: Train Loss: 7.1308, Train Acc: 0.52%, Val Loss: 7.5731, Val Acc: 0.27%\n",
            "Epoch 11: Train Loss: 7.0978, Train Acc: 0.45%, Val Loss: 7.5700, Val Acc: 0.18%\n",
            "Epoch 12: Train Loss: 7.0649, Train Acc: 0.46%, Val Loss: 7.5638, Val Acc: 0.27%\n",
            "Epoch 13: Train Loss: 7.0330, Train Acc: 0.46%, Val Loss: 7.5624, Val Acc: 0.31%\n",
            "Epoch 14: Train Loss: 7.0021, Train Acc: 0.59%, Val Loss: 7.5611, Val Acc: 0.13%\n",
            "Epoch 15: Train Loss: 6.9767, Train Acc: 0.53%, Val Loss: 7.5580, Val Acc: 0.31%\n",
            "Epoch 16: Train Loss: 6.9461, Train Acc: 0.59%, Val Loss: 7.5600, Val Acc: 0.36%\n",
            "Epoch 17: Train Loss: 6.9252, Train Acc: 0.78%, Val Loss: 7.5598, Val Acc: 0.22%\n",
            "Epoch 18: Train Loss: 6.9014, Train Acc: 0.73%, Val Loss: 7.5603, Val Acc: 0.22%\n",
            "Epoch 19: Train Loss: 6.8784, Train Acc: 0.72%, Val Loss: 7.5627, Val Acc: 0.22%\n",
            "Epoch 20: Train Loss: 6.8540, Train Acc: 0.83%, Val Loss: 7.5657, Val Acc: 0.22%\n",
            "Epoch 21: Train Loss: 6.8330, Train Acc: 0.83%, Val Loss: 7.5677, Val Acc: 0.22%\n",
            "Epoch 22: Train Loss: 6.8118, Train Acc: 0.75%, Val Loss: 7.5740, Val Acc: 0.22%\n",
            "Epoch 23: Train Loss: 6.7905, Train Acc: 0.99%, Val Loss: 7.5772, Val Acc: 0.27%\n",
            "Epoch 24: Train Loss: 6.7705, Train Acc: 0.90%, Val Loss: 7.5843, Val Acc: 0.27%\n",
            "Epoch 25: Train Loss: 6.7505, Train Acc: 0.94%, Val Loss: 7.5897, Val Acc: 0.22%\n",
            "Epoch 26: Train Loss: 6.7358, Train Acc: 0.93%, Val Loss: 7.5947, Val Acc: 0.27%\n",
            "Epoch 27: Train Loss: 6.7160, Train Acc: 1.11%, Val Loss: 7.6006, Val Acc: 0.27%\n",
            "Epoch 28: Train Loss: 6.6987, Train Acc: 0.97%, Val Loss: 7.6068, Val Acc: 0.22%\n",
            "Epoch 29: Train Loss: 6.6830, Train Acc: 1.20%, Val Loss: 7.6146, Val Acc: 0.27%\n",
            "Epoch 30: Train Loss: 6.6640, Train Acc: 1.09%, Val Loss: 7.6250, Val Acc: 0.27%\n",
            "Epoch 31: Train Loss: 6.6488, Train Acc: 1.12%, Val Loss: 7.6290, Val Acc: 0.18%\n",
            "Epoch 32: Train Loss: 6.6315, Train Acc: 1.25%, Val Loss: 7.6396, Val Acc: 0.18%\n",
            "Epoch 33: Train Loss: 6.6175, Train Acc: 1.32%, Val Loss: 7.6480, Val Acc: 0.22%\n",
            "Epoch 34: Train Loss: 6.6049, Train Acc: 1.26%, Val Loss: 7.6563, Val Acc: 0.22%\n",
            "Epoch 35: Train Loss: 6.5890, Train Acc: 1.36%, Val Loss: 7.6658, Val Acc: 0.22%\n",
            "Epoch 36: Train Loss: 6.5730, Train Acc: 1.37%, Val Loss: 7.6734, Val Acc: 0.22%\n",
            "Epoch 37: Train Loss: 6.5622, Train Acc: 1.36%, Val Loss: 7.6826, Val Acc: 0.27%\n",
            "Epoch 38: Train Loss: 6.5498, Train Acc: 1.47%, Val Loss: 7.6899, Val Acc: 0.27%\n",
            "Epoch 39: Train Loss: 6.5352, Train Acc: 1.50%, Val Loss: 7.7006, Val Acc: 0.31%\n",
            "Epoch 40: Train Loss: 6.5243, Train Acc: 1.41%, Val Loss: 7.7107, Val Acc: 0.31%\n",
            "Epoch 41: Train Loss: 6.5172, Train Acc: 1.38%, Val Loss: 7.7205, Val Acc: 0.31%\n",
            "Epoch 42: Train Loss: 6.4985, Train Acc: 1.47%, Val Loss: 7.7289, Val Acc: 0.31%\n",
            "Epoch 43: Train Loss: 6.4891, Train Acc: 1.43%, Val Loss: 7.7352, Val Acc: 0.36%\n",
            "Epoch 44: Train Loss: 6.4764, Train Acc: 1.54%, Val Loss: 7.7449, Val Acc: 0.27%\n",
            "Epoch 45: Train Loss: 6.4695, Train Acc: 1.59%, Val Loss: 7.7530, Val Acc: 0.31%\n",
            "Epoch 46: Train Loss: 6.4596, Train Acc: 1.54%, Val Loss: 7.7656, Val Acc: 0.36%\n",
            "Epoch 47: Train Loss: 6.4445, Train Acc: 1.67%, Val Loss: 7.7729, Val Acc: 0.40%\n",
            "Epoch 48: Train Loss: 6.4351, Train Acc: 1.65%, Val Loss: 7.7803, Val Acc: 0.31%\n",
            "Epoch 49: Train Loss: 6.4232, Train Acc: 1.64%, Val Loss: 7.7921, Val Acc: 0.36%\n",
            "Epoch 50: Train Loss: 6.4098, Train Acc: 1.73%, Val Loss: 7.8025, Val Acc: 0.36%\n",
            "Validation Accuracy: 0.3551\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6132, Train Acc: 0.07%, Val Loss: 7.5824, Val Acc: 0.09%\n",
            "Epoch 2: Train Loss: 7.4743, Train Acc: 0.12%, Val Loss: 7.5069, Val Acc: 0.09%\n",
            "Epoch 3: Train Loss: 7.2657, Train Acc: 0.29%, Val Loss: 7.4818, Val Acc: 0.18%\n",
            "Epoch 4: Train Loss: 7.1085, Train Acc: 0.40%, Val Loss: 7.5197, Val Acc: 0.22%\n",
            "Epoch 5: Train Loss: 6.9824, Train Acc: 0.37%, Val Loss: 7.5480, Val Acc: 0.22%\n",
            "Epoch 6: Train Loss: 6.8770, Train Acc: 0.58%, Val Loss: 7.5707, Val Acc: 0.31%\n",
            "Epoch 7: Train Loss: 6.7847, Train Acc: 0.73%, Val Loss: 7.6094, Val Acc: 0.49%\n",
            "Epoch 8: Train Loss: 6.7009, Train Acc: 0.73%, Val Loss: 7.6847, Val Acc: 0.31%\n",
            "Epoch 9: Train Loss: 6.6192, Train Acc: 0.81%, Val Loss: 7.7200, Val Acc: 0.36%\n",
            "Epoch 10: Train Loss: 6.5454, Train Acc: 0.97%, Val Loss: 7.7737, Val Acc: 0.22%\n",
            "Epoch 11: Train Loss: 6.4799, Train Acc: 1.07%, Val Loss: 7.8287, Val Acc: 0.36%\n",
            "Epoch 12: Train Loss: 6.4203, Train Acc: 1.31%, Val Loss: 7.8776, Val Acc: 0.31%\n",
            "Epoch 13: Train Loss: 6.3608, Train Acc: 1.24%, Val Loss: 7.9443, Val Acc: 0.27%\n",
            "Epoch 14: Train Loss: 6.2973, Train Acc: 1.54%, Val Loss: 8.0110, Val Acc: 0.44%\n",
            "Epoch 15: Train Loss: 6.2513, Train Acc: 1.32%, Val Loss: 8.0701, Val Acc: 0.53%\n",
            "Epoch 16: Train Loss: 6.1935, Train Acc: 1.66%, Val Loss: 8.1204, Val Acc: 0.53%\n",
            "Epoch 17: Train Loss: 6.1412, Train Acc: 1.84%, Val Loss: 8.1794, Val Acc: 0.58%\n",
            "Epoch 18: Train Loss: 6.1024, Train Acc: 2.01%, Val Loss: 8.2287, Val Acc: 0.62%\n",
            "Epoch 19: Train Loss: 6.0546, Train Acc: 2.12%, Val Loss: 8.2914, Val Acc: 0.58%\n",
            "Epoch 20: Train Loss: 6.0089, Train Acc: 2.19%, Val Loss: 8.3145, Val Acc: 0.58%\n",
            "Epoch 21: Train Loss: 5.9618, Train Acc: 2.39%, Val Loss: 8.4098, Val Acc: 0.62%\n",
            "Epoch 22: Train Loss: 5.9319, Train Acc: 2.55%, Val Loss: 8.4219, Val Acc: 0.67%\n",
            "Epoch 23: Train Loss: 5.8814, Train Acc: 2.68%, Val Loss: 8.4730, Val Acc: 0.62%\n",
            "Epoch 24: Train Loss: 5.8468, Train Acc: 2.88%, Val Loss: 8.5601, Val Acc: 0.67%\n",
            "Epoch 25: Train Loss: 5.8078, Train Acc: 2.94%, Val Loss: 8.6000, Val Acc: 0.71%\n",
            "Epoch 26: Train Loss: 5.7817, Train Acc: 2.98%, Val Loss: 8.6489, Val Acc: 0.62%\n",
            "Epoch 27: Train Loss: 5.7396, Train Acc: 3.31%, Val Loss: 8.7313, Val Acc: 0.58%\n",
            "Epoch 28: Train Loss: 5.7062, Train Acc: 3.40%, Val Loss: 8.7403, Val Acc: 0.84%\n",
            "Epoch 29: Train Loss: 5.6676, Train Acc: 3.63%, Val Loss: 8.7966, Val Acc: 0.62%\n",
            "Epoch 30: Train Loss: 5.6412, Train Acc: 3.44%, Val Loss: 8.8328, Val Acc: 0.75%\n",
            "Epoch 31: Train Loss: 5.6001, Train Acc: 4.10%, Val Loss: 8.9038, Val Acc: 0.75%\n",
            "Epoch 32: Train Loss: 5.5784, Train Acc: 4.04%, Val Loss: 8.9825, Val Acc: 0.71%\n",
            "Epoch 33: Train Loss: 5.5430, Train Acc: 3.99%, Val Loss: 8.9415, Val Acc: 0.75%\n",
            "Epoch 34: Train Loss: 5.5115, Train Acc: 4.23%, Val Loss: 9.0187, Val Acc: 0.71%\n",
            "Epoch 35: Train Loss: 5.4812, Train Acc: 4.60%, Val Loss: 9.0690, Val Acc: 0.67%\n",
            "Epoch 36: Train Loss: 5.4560, Train Acc: 4.74%, Val Loss: 9.1024, Val Acc: 0.75%\n",
            "Epoch 37: Train Loss: 5.4297, Train Acc: 4.47%, Val Loss: 9.1455, Val Acc: 0.71%\n",
            "Epoch 38: Train Loss: 5.3961, Train Acc: 4.93%, Val Loss: 9.2045, Val Acc: 0.71%\n",
            "Epoch 39: Train Loss: 5.3849, Train Acc: 4.87%, Val Loss: 9.2016, Val Acc: 0.67%\n",
            "Epoch 40: Train Loss: 5.3493, Train Acc: 5.03%, Val Loss: 9.2450, Val Acc: 0.84%\n",
            "Epoch 41: Train Loss: 5.3359, Train Acc: 5.08%, Val Loss: 9.2886, Val Acc: 0.80%\n",
            "Epoch 42: Train Loss: 5.3065, Train Acc: 5.28%, Val Loss: 9.3353, Val Acc: 0.89%\n",
            "Epoch 43: Train Loss: 5.2765, Train Acc: 5.27%, Val Loss: 9.3660, Val Acc: 0.71%\n",
            "Epoch 44: Train Loss: 5.2620, Train Acc: 5.47%, Val Loss: 9.3587, Val Acc: 0.84%\n",
            "Epoch 45: Train Loss: 5.2401, Train Acc: 5.51%, Val Loss: 9.4357, Val Acc: 0.80%\n",
            "Epoch 46: Train Loss: 5.2144, Train Acc: 5.65%, Val Loss: 9.4816, Val Acc: 0.84%\n",
            "Epoch 47: Train Loss: 5.1945, Train Acc: 5.92%, Val Loss: 9.4931, Val Acc: 0.80%\n",
            "Epoch 48: Train Loss: 5.1711, Train Acc: 6.09%, Val Loss: 9.5049, Val Acc: 0.80%\n",
            "Epoch 49: Train Loss: 5.1604, Train Acc: 5.83%, Val Loss: 9.5547, Val Acc: 0.75%\n",
            "Epoch 50: Train Loss: 5.1414, Train Acc: 6.32%, Val Loss: 9.5592, Val Acc: 0.93%\n",
            "Validation Accuracy: 0.9321\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6122, Train Acc: 0.04%, Val Loss: 7.5736, Val Acc: 0.13%\n",
            "Epoch 2: Train Loss: 7.4622, Train Acc: 0.19%, Val Loss: 7.4864, Val Acc: 0.13%\n",
            "Epoch 3: Train Loss: 7.2715, Train Acc: 0.14%, Val Loss: 7.4569, Val Acc: 0.22%\n",
            "Epoch 4: Train Loss: 7.1448, Train Acc: 0.35%, Val Loss: 7.4450, Val Acc: 0.22%\n",
            "Epoch 5: Train Loss: 7.0537, Train Acc: 0.34%, Val Loss: 7.4406, Val Acc: 0.27%\n",
            "Epoch 6: Train Loss: 6.9686, Train Acc: 0.47%, Val Loss: 7.4451, Val Acc: 0.27%\n",
            "Epoch 7: Train Loss: 6.8903, Train Acc: 0.53%, Val Loss: 7.4537, Val Acc: 0.18%\n",
            "Epoch 8: Train Loss: 6.8231, Train Acc: 0.55%, Val Loss: 7.4587, Val Acc: 0.18%\n",
            "Epoch 9: Train Loss: 6.7610, Train Acc: 0.63%, Val Loss: 7.4888, Val Acc: 0.13%\n",
            "Epoch 10: Train Loss: 6.6937, Train Acc: 0.81%, Val Loss: 7.5165, Val Acc: 0.18%\n",
            "Epoch 11: Train Loss: 6.6371, Train Acc: 0.90%, Val Loss: 7.5390, Val Acc: 0.13%\n",
            "Epoch 12: Train Loss: 6.5836, Train Acc: 0.89%, Val Loss: 7.5622, Val Acc: 0.31%\n",
            "Epoch 13: Train Loss: 6.5288, Train Acc: 0.94%, Val Loss: 7.5899, Val Acc: 0.27%\n",
            "Epoch 14: Train Loss: 6.4941, Train Acc: 1.03%, Val Loss: 7.6271, Val Acc: 0.27%\n",
            "Epoch 15: Train Loss: 6.4374, Train Acc: 1.35%, Val Loss: 7.6352, Val Acc: 0.53%\n",
            "Epoch 16: Train Loss: 6.3997, Train Acc: 1.29%, Val Loss: 7.6734, Val Acc: 0.40%\n",
            "Epoch 17: Train Loss: 6.3627, Train Acc: 1.23%, Val Loss: 7.6889, Val Acc: 0.36%\n",
            "Epoch 18: Train Loss: 6.3194, Train Acc: 1.30%, Val Loss: 7.7236, Val Acc: 0.49%\n",
            "Epoch 19: Train Loss: 6.2881, Train Acc: 1.65%, Val Loss: 7.7340, Val Acc: 0.44%\n",
            "Epoch 20: Train Loss: 6.2670, Train Acc: 1.66%, Val Loss: 7.7705, Val Acc: 0.44%\n",
            "Epoch 21: Train Loss: 6.2262, Train Acc: 1.98%, Val Loss: 7.7931, Val Acc: 0.36%\n",
            "Epoch 22: Train Loss: 6.1925, Train Acc: 1.85%, Val Loss: 7.8081, Val Acc: 0.44%\n",
            "Epoch 23: Train Loss: 6.1583, Train Acc: 1.95%, Val Loss: 7.8268, Val Acc: 0.49%\n",
            "Epoch 24: Train Loss: 6.1292, Train Acc: 2.09%, Val Loss: 7.8577, Val Acc: 0.44%\n",
            "Epoch 25: Train Loss: 6.1039, Train Acc: 2.15%, Val Loss: 7.8678, Val Acc: 0.40%\n",
            "Epoch 26: Train Loss: 6.0765, Train Acc: 2.01%, Val Loss: 7.8902, Val Acc: 0.53%\n",
            "Epoch 27: Train Loss: 6.0414, Train Acc: 2.12%, Val Loss: 7.9152, Val Acc: 0.80%\n",
            "Epoch 28: Train Loss: 6.0104, Train Acc: 2.35%, Val Loss: 7.9288, Val Acc: 0.67%\n",
            "Epoch 29: Train Loss: 5.9923, Train Acc: 2.31%, Val Loss: 7.9598, Val Acc: 0.49%\n",
            "Epoch 30: Train Loss: 5.9643, Train Acc: 2.29%, Val Loss: 7.9527, Val Acc: 0.53%\n",
            "Epoch 31: Train Loss: 5.9319, Train Acc: 2.63%, Val Loss: 7.9590, Val Acc: 0.62%\n",
            "Epoch 32: Train Loss: 5.9187, Train Acc: 2.68%, Val Loss: 7.9849, Val Acc: 0.71%\n",
            "Epoch 33: Train Loss: 5.8928, Train Acc: 2.68%, Val Loss: 7.9977, Val Acc: 0.58%\n",
            "Epoch 34: Train Loss: 5.8714, Train Acc: 2.73%, Val Loss: 8.0189, Val Acc: 0.62%\n",
            "Epoch 35: Train Loss: 5.8517, Train Acc: 2.81%, Val Loss: 8.0212, Val Acc: 0.53%\n",
            "Epoch 36: Train Loss: 5.8173, Train Acc: 2.96%, Val Loss: 8.0384, Val Acc: 0.58%\n",
            "Epoch 37: Train Loss: 5.7949, Train Acc: 3.22%, Val Loss: 8.0587, Val Acc: 0.75%\n",
            "Epoch 38: Train Loss: 5.7808, Train Acc: 3.28%, Val Loss: 8.0828, Val Acc: 0.62%\n",
            "Epoch 39: Train Loss: 5.7624, Train Acc: 3.49%, Val Loss: 8.0752, Val Acc: 0.71%\n",
            "Epoch 40: Train Loss: 5.7359, Train Acc: 3.46%, Val Loss: 8.0877, Val Acc: 0.53%\n",
            "Epoch 41: Train Loss: 5.7082, Train Acc: 3.37%, Val Loss: 8.1249, Val Acc: 0.49%\n",
            "Epoch 42: Train Loss: 5.6981, Train Acc: 3.71%, Val Loss: 8.1268, Val Acc: 0.67%\n",
            "Epoch 43: Train Loss: 5.6771, Train Acc: 3.77%, Val Loss: 8.1308, Val Acc: 0.62%\n",
            "Epoch 44: Train Loss: 5.6498, Train Acc: 3.89%, Val Loss: 8.1467, Val Acc: 0.62%\n",
            "Epoch 45: Train Loss: 5.6359, Train Acc: 4.04%, Val Loss: 8.1556, Val Acc: 0.58%\n",
            "Epoch 46: Train Loss: 5.6190, Train Acc: 3.99%, Val Loss: 8.1720, Val Acc: 0.67%\n",
            "Epoch 47: Train Loss: 5.6053, Train Acc: 4.10%, Val Loss: 8.1916, Val Acc: 0.71%\n",
            "Epoch 48: Train Loss: 5.5828, Train Acc: 4.14%, Val Loss: 8.1863, Val Acc: 0.62%\n",
            "Epoch 49: Train Loss: 5.5767, Train Acc: 4.03%, Val Loss: 8.1968, Val Acc: 0.80%\n",
            "Epoch 50: Train Loss: 5.5513, Train Acc: 3.80%, Val Loss: 8.2072, Val Acc: 0.67%\n",
            "Validation Accuracy: 0.6658\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.0005, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6127, Train Acc: 0.05%, Val Loss: 7.5773, Val Acc: 0.18%\n",
            "Epoch 2: Train Loss: 7.4546, Train Acc: 0.24%, Val Loss: 7.5054, Val Acc: 0.13%\n",
            "Epoch 3: Train Loss: 7.2337, Train Acc: 0.37%, Val Loss: 7.4988, Val Acc: 0.13%\n",
            "Epoch 4: Train Loss: 7.0849, Train Acc: 0.45%, Val Loss: 7.5235, Val Acc: 0.13%\n",
            "Epoch 5: Train Loss: 6.9678, Train Acc: 0.55%, Val Loss: 7.5494, Val Acc: 0.22%\n",
            "Epoch 6: Train Loss: 6.8641, Train Acc: 0.57%, Val Loss: 7.5717, Val Acc: 0.36%\n",
            "Epoch 7: Train Loss: 6.7671, Train Acc: 0.79%, Val Loss: 7.6199, Val Acc: 0.31%\n",
            "Epoch 8: Train Loss: 6.6785, Train Acc: 0.95%, Val Loss: 7.6591, Val Acc: 0.27%\n",
            "Epoch 9: Train Loss: 6.5955, Train Acc: 0.94%, Val Loss: 7.7155, Val Acc: 0.40%\n",
            "Epoch 10: Train Loss: 6.5264, Train Acc: 1.09%, Val Loss: 7.7766, Val Acc: 0.31%\n",
            "Epoch 11: Train Loss: 6.4585, Train Acc: 1.07%, Val Loss: 7.8370, Val Acc: 0.31%\n",
            "Epoch 12: Train Loss: 6.3900, Train Acc: 1.50%, Val Loss: 7.9047, Val Acc: 0.44%\n",
            "Epoch 13: Train Loss: 6.3378, Train Acc: 1.55%, Val Loss: 7.9576, Val Acc: 0.40%\n",
            "Epoch 14: Train Loss: 6.2849, Train Acc: 1.55%, Val Loss: 8.0151, Val Acc: 0.44%\n",
            "Epoch 15: Train Loss: 6.2348, Train Acc: 1.74%, Val Loss: 8.0764, Val Acc: 0.40%\n",
            "Epoch 16: Train Loss: 6.1878, Train Acc: 2.02%, Val Loss: 8.1462, Val Acc: 0.40%\n",
            "Epoch 17: Train Loss: 6.1388, Train Acc: 1.95%, Val Loss: 8.2162, Val Acc: 0.40%\n",
            "Epoch 18: Train Loss: 6.0892, Train Acc: 2.27%, Val Loss: 8.2795, Val Acc: 0.36%\n",
            "Epoch 19: Train Loss: 6.0523, Train Acc: 2.37%, Val Loss: 8.3293, Val Acc: 0.44%\n",
            "Epoch 20: Train Loss: 6.0070, Train Acc: 2.42%, Val Loss: 8.3879, Val Acc: 0.40%\n",
            "Epoch 21: Train Loss: 5.9608, Train Acc: 2.71%, Val Loss: 8.4585, Val Acc: 0.40%\n",
            "Epoch 22: Train Loss: 5.9263, Train Acc: 2.85%, Val Loss: 8.4880, Val Acc: 0.49%\n",
            "Epoch 23: Train Loss: 5.8961, Train Acc: 2.95%, Val Loss: 8.5712, Val Acc: 0.44%\n",
            "Epoch 24: Train Loss: 5.8510, Train Acc: 3.19%, Val Loss: 8.6278, Val Acc: 0.49%\n",
            "Epoch 25: Train Loss: 5.8080, Train Acc: 3.49%, Val Loss: 8.6737, Val Acc: 0.44%\n",
            "Epoch 26: Train Loss: 5.7770, Train Acc: 3.39%, Val Loss: 8.7137, Val Acc: 0.40%\n",
            "Epoch 27: Train Loss: 5.7414, Train Acc: 3.61%, Val Loss: 8.7708, Val Acc: 0.44%\n",
            "Epoch 28: Train Loss: 5.7061, Train Acc: 3.78%, Val Loss: 8.8528, Val Acc: 0.44%\n",
            "Epoch 29: Train Loss: 5.6663, Train Acc: 3.93%, Val Loss: 8.8748, Val Acc: 0.44%\n",
            "Epoch 30: Train Loss: 5.6445, Train Acc: 4.07%, Val Loss: 8.9478, Val Acc: 0.40%\n",
            "Epoch 31: Train Loss: 5.6010, Train Acc: 4.13%, Val Loss: 8.9891, Val Acc: 0.53%\n",
            "Epoch 32: Train Loss: 5.5710, Train Acc: 4.34%, Val Loss: 9.0525, Val Acc: 0.44%\n",
            "Epoch 33: Train Loss: 5.5349, Train Acc: 4.80%, Val Loss: 9.1135, Val Acc: 0.49%\n",
            "Epoch 34: Train Loss: 5.5017, Train Acc: 4.47%, Val Loss: 9.1561, Val Acc: 0.58%\n",
            "Epoch 35: Train Loss: 5.4732, Train Acc: 5.15%, Val Loss: 9.1936, Val Acc: 0.40%\n",
            "Epoch 36: Train Loss: 5.4369, Train Acc: 5.29%, Val Loss: 9.2510, Val Acc: 0.40%\n",
            "Epoch 37: Train Loss: 5.4120, Train Acc: 5.22%, Val Loss: 9.2806, Val Acc: 0.49%\n",
            "Epoch 38: Train Loss: 5.3937, Train Acc: 5.11%, Val Loss: 9.3289, Val Acc: 0.44%\n",
            "Epoch 39: Train Loss: 5.3581, Train Acc: 5.44%, Val Loss: 9.3713, Val Acc: 0.49%\n",
            "Epoch 40: Train Loss: 5.3276, Train Acc: 6.26%, Val Loss: 9.4167, Val Acc: 0.40%\n",
            "Epoch 41: Train Loss: 5.3039, Train Acc: 5.93%, Val Loss: 9.4606, Val Acc: 0.44%\n",
            "Epoch 42: Train Loss: 5.2660, Train Acc: 6.15%, Val Loss: 9.5235, Val Acc: 0.53%\n",
            "Epoch 43: Train Loss: 5.2492, Train Acc: 5.85%, Val Loss: 9.5639, Val Acc: 0.44%\n",
            "Epoch 44: Train Loss: 5.2201, Train Acc: 6.68%, Val Loss: 9.5854, Val Acc: 0.44%\n",
            "Epoch 45: Train Loss: 5.2009, Train Acc: 6.58%, Val Loss: 9.6226, Val Acc: 0.49%\n",
            "Epoch 46: Train Loss: 5.1552, Train Acc: 7.24%, Val Loss: 9.6872, Val Acc: 0.53%\n",
            "Epoch 47: Train Loss: 5.1412, Train Acc: 7.09%, Val Loss: 9.6915, Val Acc: 0.58%\n",
            "Epoch 48: Train Loss: 5.1202, Train Acc: 7.30%, Val Loss: 9.7274, Val Acc: 0.58%\n",
            "Epoch 49: Train Loss: 5.0914, Train Acc: 7.17%, Val Loss: 9.7769, Val Acc: 0.58%\n",
            "Epoch 50: Train Loss: 5.0600, Train Acc: 7.47%, Val Loss: 9.8235, Val Acc: 0.53%\n",
            "Validation Accuracy: 0.5326\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.0005, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6132, Train Acc: 0.06%, Val Loss: 7.5866, Val Acc: 0.09%\n",
            "Epoch 2: Train Loss: 7.4885, Train Acc: 0.19%, Val Loss: 7.5255, Val Acc: 0.13%\n",
            "Epoch 3: Train Loss: 7.2931, Train Acc: 0.23%, Val Loss: 7.4997, Val Acc: 0.09%\n",
            "Epoch 4: Train Loss: 7.1496, Train Acc: 0.41%, Val Loss: 7.4932, Val Acc: 0.13%\n",
            "Epoch 5: Train Loss: 7.0432, Train Acc: 0.42%, Val Loss: 7.4833, Val Acc: 0.13%\n",
            "Epoch 6: Train Loss: 6.9584, Train Acc: 0.43%, Val Loss: 7.4888, Val Acc: 0.22%\n",
            "Epoch 7: Train Loss: 6.8799, Train Acc: 0.54%, Val Loss: 7.4891, Val Acc: 0.22%\n",
            "Epoch 8: Train Loss: 6.8113, Train Acc: 0.82%, Val Loss: 7.5097, Val Acc: 0.27%\n",
            "Epoch 9: Train Loss: 6.7499, Train Acc: 0.81%, Val Loss: 7.5267, Val Acc: 0.27%\n",
            "Epoch 10: Train Loss: 6.6959, Train Acc: 0.83%, Val Loss: 7.5389, Val Acc: 0.27%\n",
            "Epoch 11: Train Loss: 6.6360, Train Acc: 0.90%, Val Loss: 7.5662, Val Acc: 0.27%\n",
            "Epoch 12: Train Loss: 6.5873, Train Acc: 1.02%, Val Loss: 7.5923, Val Acc: 0.40%\n",
            "Epoch 13: Train Loss: 6.5393, Train Acc: 1.02%, Val Loss: 7.6162, Val Acc: 0.36%\n",
            "Epoch 14: Train Loss: 6.4859, Train Acc: 1.17%, Val Loss: 7.6376, Val Acc: 0.22%\n",
            "Epoch 15: Train Loss: 6.4492, Train Acc: 1.36%, Val Loss: 7.6764, Val Acc: 0.31%\n",
            "Epoch 16: Train Loss: 6.4072, Train Acc: 1.52%, Val Loss: 7.6974, Val Acc: 0.40%\n",
            "Epoch 17: Train Loss: 6.3699, Train Acc: 1.40%, Val Loss: 7.7216, Val Acc: 0.40%\n",
            "Epoch 18: Train Loss: 6.3288, Train Acc: 1.62%, Val Loss: 7.7499, Val Acc: 0.31%\n",
            "Epoch 19: Train Loss: 6.2853, Train Acc: 1.59%, Val Loss: 7.7826, Val Acc: 0.40%\n",
            "Epoch 20: Train Loss: 6.2495, Train Acc: 1.83%, Val Loss: 7.8138, Val Acc: 0.22%\n",
            "Epoch 21: Train Loss: 6.2160, Train Acc: 2.09%, Val Loss: 7.8200, Val Acc: 0.36%\n",
            "Epoch 22: Train Loss: 6.1753, Train Acc: 1.88%, Val Loss: 7.8570, Val Acc: 0.40%\n",
            "Epoch 23: Train Loss: 6.1507, Train Acc: 2.02%, Val Loss: 7.8784, Val Acc: 0.49%\n",
            "Epoch 24: Train Loss: 6.1166, Train Acc: 2.38%, Val Loss: 7.8975, Val Acc: 0.58%\n",
            "Epoch 25: Train Loss: 6.0922, Train Acc: 2.33%, Val Loss: 7.9284, Val Acc: 0.40%\n",
            "Epoch 26: Train Loss: 6.0517, Train Acc: 2.47%, Val Loss: 7.9363, Val Acc: 0.53%\n",
            "Epoch 27: Train Loss: 6.0301, Train Acc: 2.79%, Val Loss: 7.9667, Val Acc: 0.58%\n",
            "Epoch 28: Train Loss: 6.0027, Train Acc: 2.59%, Val Loss: 7.9899, Val Acc: 0.40%\n",
            "Epoch 29: Train Loss: 5.9740, Train Acc: 2.72%, Val Loss: 7.9985, Val Acc: 0.58%\n",
            "Epoch 30: Train Loss: 5.9452, Train Acc: 2.85%, Val Loss: 8.0297, Val Acc: 0.53%\n",
            "Epoch 31: Train Loss: 5.9220, Train Acc: 3.03%, Val Loss: 8.0477, Val Acc: 0.58%\n",
            "Epoch 32: Train Loss: 5.8943, Train Acc: 2.90%, Val Loss: 8.0766, Val Acc: 0.49%\n",
            "Epoch 33: Train Loss: 5.8720, Train Acc: 3.09%, Val Loss: 8.0819, Val Acc: 0.53%\n",
            "Epoch 34: Train Loss: 5.8527, Train Acc: 3.03%, Val Loss: 8.0939, Val Acc: 0.53%\n",
            "Epoch 35: Train Loss: 5.8263, Train Acc: 3.32%, Val Loss: 8.1169, Val Acc: 0.49%\n",
            "Epoch 36: Train Loss: 5.8034, Train Acc: 3.52%, Val Loss: 8.1356, Val Acc: 0.58%\n",
            "Epoch 37: Train Loss: 5.7759, Train Acc: 3.64%, Val Loss: 8.1465, Val Acc: 0.49%\n",
            "Epoch 38: Train Loss: 5.7471, Train Acc: 3.58%, Val Loss: 8.1756, Val Acc: 0.58%\n",
            "Epoch 39: Train Loss: 5.7340, Train Acc: 3.45%, Val Loss: 8.1783, Val Acc: 0.75%\n",
            "Epoch 40: Train Loss: 5.7110, Train Acc: 3.95%, Val Loss: 8.1997, Val Acc: 0.67%\n",
            "Epoch 41: Train Loss: 5.6894, Train Acc: 3.96%, Val Loss: 8.2145, Val Acc: 0.71%\n",
            "Epoch 42: Train Loss: 5.6695, Train Acc: 4.01%, Val Loss: 8.2250, Val Acc: 0.62%\n",
            "Epoch 43: Train Loss: 5.6482, Train Acc: 3.89%, Val Loss: 8.2396, Val Acc: 0.58%\n",
            "Epoch 44: Train Loss: 5.6347, Train Acc: 4.16%, Val Loss: 8.2636, Val Acc: 0.67%\n",
            "Epoch 45: Train Loss: 5.6182, Train Acc: 4.03%, Val Loss: 8.2624, Val Acc: 0.75%\n",
            "Epoch 46: Train Loss: 5.5973, Train Acc: 4.49%, Val Loss: 8.2723, Val Acc: 0.71%\n",
            "Epoch 47: Train Loss: 5.5703, Train Acc: 4.55%, Val Loss: 8.3023, Val Acc: 0.62%\n",
            "Epoch 48: Train Loss: 5.5492, Train Acc: 4.45%, Val Loss: 8.3104, Val Acc: 0.67%\n",
            "Epoch 49: Train Loss: 5.5287, Train Acc: 4.43%, Val Loss: 8.3201, Val Acc: 0.75%\n",
            "Epoch 50: Train Loss: 5.5131, Train Acc: 4.62%, Val Loss: 8.3364, Val Acc: 0.49%\n",
            "Validation Accuracy: 0.4882\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
            "Epoch 1: Train Loss: 7.6220, Train Acc: 0.06%, Val Loss: 7.5999, Val Acc: 0.13%\n",
            "Epoch 2: Train Loss: 7.5516, Train Acc: 0.16%, Val Loss: 7.5932, Val Acc: 0.04%\n",
            "Epoch 3: Train Loss: 7.4679, Train Acc: 0.23%, Val Loss: 7.6007, Val Acc: 0.09%\n",
            "Epoch 4: Train Loss: 7.3961, Train Acc: 0.23%, Val Loss: 7.6101, Val Acc: 0.13%\n",
            "Epoch 5: Train Loss: 7.3345, Train Acc: 0.29%, Val Loss: 7.6189, Val Acc: 0.09%\n",
            "Epoch 6: Train Loss: 7.2797, Train Acc: 0.31%, Val Loss: 7.6140, Val Acc: 0.09%\n",
            "Epoch 7: Train Loss: 7.2311, Train Acc: 0.40%, Val Loss: 7.6127, Val Acc: 0.13%\n",
            "Epoch 8: Train Loss: 7.1840, Train Acc: 0.36%, Val Loss: 7.6084, Val Acc: 0.13%\n",
            "Epoch 9: Train Loss: 7.1421, Train Acc: 0.48%, Val Loss: 7.6040, Val Acc: 0.09%\n",
            "Epoch 10: Train Loss: 7.1009, Train Acc: 0.45%, Val Loss: 7.6028, Val Acc: 0.13%\n",
            "Epoch 11: Train Loss: 7.0637, Train Acc: 0.52%, Val Loss: 7.6023, Val Acc: 0.18%\n",
            "Epoch 12: Train Loss: 7.0273, Train Acc: 0.60%, Val Loss: 7.6061, Val Acc: 0.22%\n",
            "Epoch 13: Train Loss: 6.9933, Train Acc: 0.64%, Val Loss: 7.6106, Val Acc: 0.22%\n",
            "Epoch 14: Train Loss: 6.9614, Train Acc: 0.65%, Val Loss: 7.6141, Val Acc: 0.18%\n",
            "Epoch 15: Train Loss: 6.9298, Train Acc: 0.60%, Val Loss: 7.6228, Val Acc: 0.22%\n",
            "Epoch 16: Train Loss: 6.9034, Train Acc: 0.81%, Val Loss: 7.6266, Val Acc: 0.27%\n",
            "Epoch 17: Train Loss: 6.8741, Train Acc: 0.69%, Val Loss: 7.6351, Val Acc: 0.27%\n",
            "Epoch 18: Train Loss: 6.8443, Train Acc: 0.85%, Val Loss: 7.6455, Val Acc: 0.27%\n",
            "Epoch 19: Train Loss: 6.8193, Train Acc: 0.88%, Val Loss: 7.6571, Val Acc: 0.27%\n",
            "Epoch 20: Train Loss: 6.7939, Train Acc: 0.88%, Val Loss: 7.6685, Val Acc: 0.27%\n",
            "Epoch 21: Train Loss: 6.7681, Train Acc: 0.93%, Val Loss: 7.6787, Val Acc: 0.27%\n",
            "Epoch 22: Train Loss: 6.7428, Train Acc: 0.99%, Val Loss: 7.6899, Val Acc: 0.22%\n",
            "Epoch 23: Train Loss: 6.7206, Train Acc: 0.95%, Val Loss: 7.7073, Val Acc: 0.22%\n",
            "Epoch 24: Train Loss: 6.6990, Train Acc: 1.07%, Val Loss: 7.7211, Val Acc: 0.22%\n",
            "Epoch 25: Train Loss: 6.6785, Train Acc: 1.03%, Val Loss: 7.7335, Val Acc: 0.18%\n",
            "Epoch 26: Train Loss: 6.6590, Train Acc: 1.09%, Val Loss: 7.7473, Val Acc: 0.27%\n",
            "Epoch 27: Train Loss: 6.6363, Train Acc: 1.28%, Val Loss: 7.7645, Val Acc: 0.18%\n",
            "Epoch 28: Train Loss: 6.6185, Train Acc: 1.37%, Val Loss: 7.7843, Val Acc: 0.22%\n",
            "Epoch 29: Train Loss: 6.5962, Train Acc: 1.28%, Val Loss: 7.7990, Val Acc: 0.27%\n",
            "Epoch 30: Train Loss: 6.5815, Train Acc: 1.35%, Val Loss: 7.8156, Val Acc: 0.31%\n",
            "Epoch 31: Train Loss: 6.5632, Train Acc: 1.54%, Val Loss: 7.8353, Val Acc: 0.31%\n",
            "Epoch 32: Train Loss: 6.5476, Train Acc: 1.37%, Val Loss: 7.8520, Val Acc: 0.31%\n",
            "Epoch 33: Train Loss: 6.5289, Train Acc: 1.56%, Val Loss: 7.8668, Val Acc: 0.31%\n",
            "Epoch 34: Train Loss: 6.5143, Train Acc: 1.53%, Val Loss: 7.8855, Val Acc: 0.36%\n",
            "Epoch 35: Train Loss: 6.4932, Train Acc: 1.53%, Val Loss: 7.9084, Val Acc: 0.31%\n",
            "Epoch 36: Train Loss: 6.4820, Train Acc: 1.56%, Val Loss: 7.9234, Val Acc: 0.36%\n",
            "Epoch 37: Train Loss: 6.4702, Train Acc: 1.70%, Val Loss: 7.9429, Val Acc: 0.36%\n",
            "Epoch 38: Train Loss: 6.4513, Train Acc: 1.77%, Val Loss: 7.9574, Val Acc: 0.36%\n",
            "Epoch 39: Train Loss: 6.4401, Train Acc: 1.68%, Val Loss: 7.9792, Val Acc: 0.36%\n",
            "Epoch 40: Train Loss: 6.4310, Train Acc: 1.68%, Val Loss: 7.9966, Val Acc: 0.40%\n",
            "Epoch 41: Train Loss: 6.4103, Train Acc: 1.94%, Val Loss: 8.0123, Val Acc: 0.40%\n",
            "Epoch 42: Train Loss: 6.4004, Train Acc: 1.72%, Val Loss: 8.0335, Val Acc: 0.44%\n",
            "Epoch 43: Train Loss: 6.3815, Train Acc: 1.92%, Val Loss: 8.0514, Val Acc: 0.40%\n",
            "Epoch 44: Train Loss: 6.3744, Train Acc: 1.84%, Val Loss: 8.0704, Val Acc: 0.44%\n",
            "Epoch 45: Train Loss: 6.3646, Train Acc: 1.94%, Val Loss: 8.0923, Val Acc: 0.44%\n",
            "Epoch 46: Train Loss: 6.3477, Train Acc: 1.89%, Val Loss: 8.1130, Val Acc: 0.44%\n",
            "Epoch 47: Train Loss: 6.3392, Train Acc: 1.77%, Val Loss: 8.1238, Val Acc: 0.44%\n",
            "Epoch 48: Train Loss: 6.3246, Train Acc: 2.00%, Val Loss: 8.1488, Val Acc: 0.44%\n",
            "Epoch 49: Train Loss: 6.3174, Train Acc: 2.00%, Val Loss: 8.1587, Val Acc: 0.40%\n",
            "Epoch 50: Train Loss: 6.3061, Train Acc: 2.12%, Val Loss: 8.1786, Val Acc: 0.44%\n",
            "Validation Accuracy: 0.4439\n",
            "Training with parameters: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
            "Epoch 1: Train Loss: 7.6225, Train Acc: 0.02%, Val Loss: 7.6008, Val Acc: 0.00%\n",
            "Epoch 2: Train Loss: 7.5612, Train Acc: 0.10%, Val Loss: 7.5938, Val Acc: 0.04%\n",
            "Epoch 3: Train Loss: 7.4926, Train Acc: 0.20%, Val Loss: 7.5994, Val Acc: 0.09%\n",
            "Epoch 4: Train Loss: 7.4283, Train Acc: 0.19%, Val Loss: 7.6054, Val Acc: 0.13%\n",
            "Epoch 5: Train Loss: 7.3692, Train Acc: 0.35%, Val Loss: 7.6055, Val Acc: 0.13%\n",
            "Epoch 6: Train Loss: 7.3136, Train Acc: 0.28%, Val Loss: 7.6030, Val Acc: 0.09%\n",
            "Epoch 7: Train Loss: 7.2656, Train Acc: 0.40%, Val Loss: 7.5961, Val Acc: 0.18%\n",
            "Epoch 8: Train Loss: 7.2236, Train Acc: 0.37%, Val Loss: 7.5857, Val Acc: 0.13%\n",
            "Epoch 9: Train Loss: 7.1847, Train Acc: 0.42%, Val Loss: 7.5804, Val Acc: 0.09%\n",
            "Epoch 10: Train Loss: 7.1473, Train Acc: 0.45%, Val Loss: 7.5758, Val Acc: 0.13%\n",
            "Epoch 11: Train Loss: 7.1133, Train Acc: 0.43%, Val Loss: 7.5687, Val Acc: 0.18%\n",
            "Epoch 12: Train Loss: 7.0777, Train Acc: 0.58%, Val Loss: 7.5634, Val Acc: 0.18%\n",
            "Epoch 13: Train Loss: 7.0483, Train Acc: 0.71%, Val Loss: 7.5609, Val Acc: 0.18%\n",
            "Epoch 14: Train Loss: 7.0183, Train Acc: 0.69%, Val Loss: 7.5593, Val Acc: 0.13%\n",
            "Epoch 15: Train Loss: 6.9895, Train Acc: 0.55%, Val Loss: 7.5559, Val Acc: 0.13%\n",
            "Epoch 16: Train Loss: 6.9635, Train Acc: 0.61%, Val Loss: 7.5547, Val Acc: 0.13%\n",
            "Epoch 17: Train Loss: 6.9386, Train Acc: 0.67%, Val Loss: 7.5542, Val Acc: 0.18%\n",
            "Epoch 18: Train Loss: 6.9121, Train Acc: 0.69%, Val Loss: 7.5557, Val Acc: 0.18%\n",
            "Epoch 19: Train Loss: 6.8873, Train Acc: 0.78%, Val Loss: 7.5586, Val Acc: 0.18%\n",
            "Epoch 20: Train Loss: 6.8680, Train Acc: 0.75%, Val Loss: 7.5606, Val Acc: 0.22%\n",
            "Epoch 21: Train Loss: 6.8438, Train Acc: 0.87%, Val Loss: 7.5613, Val Acc: 0.22%\n",
            "Epoch 22: Train Loss: 6.8212, Train Acc: 0.85%, Val Loss: 7.5640, Val Acc: 0.27%\n",
            "Epoch 23: Train Loss: 6.8044, Train Acc: 0.87%, Val Loss: 7.5699, Val Acc: 0.27%\n",
            "Epoch 24: Train Loss: 6.7834, Train Acc: 0.99%, Val Loss: 7.5743, Val Acc: 0.27%\n",
            "Epoch 25: Train Loss: 6.7624, Train Acc: 0.94%, Val Loss: 7.5799, Val Acc: 0.22%\n",
            "Epoch 26: Train Loss: 6.7439, Train Acc: 1.00%, Val Loss: 7.5863, Val Acc: 0.27%\n",
            "Epoch 27: Train Loss: 6.7243, Train Acc: 0.95%, Val Loss: 7.5919, Val Acc: 0.31%\n",
            "Epoch 28: Train Loss: 6.7087, Train Acc: 0.93%, Val Loss: 7.6020, Val Acc: 0.27%\n",
            "Epoch 29: Train Loss: 6.6914, Train Acc: 1.00%, Val Loss: 7.6060, Val Acc: 0.22%\n",
            "Epoch 30: Train Loss: 6.6775, Train Acc: 1.06%, Val Loss: 7.6136, Val Acc: 0.18%\n",
            "Epoch 31: Train Loss: 6.6572, Train Acc: 1.29%, Val Loss: 7.6221, Val Acc: 0.27%\n",
            "Epoch 32: Train Loss: 6.6429, Train Acc: 1.00%, Val Loss: 7.6318, Val Acc: 0.18%\n",
            "Epoch 33: Train Loss: 6.6276, Train Acc: 1.26%, Val Loss: 7.6411, Val Acc: 0.31%\n",
            "Epoch 34: Train Loss: 6.6133, Train Acc: 1.29%, Val Loss: 7.6495, Val Acc: 0.31%\n",
            "Epoch 35: Train Loss: 6.5999, Train Acc: 1.18%, Val Loss: 7.6606, Val Acc: 0.31%\n",
            "Epoch 36: Train Loss: 6.5863, Train Acc: 1.40%, Val Loss: 7.6682, Val Acc: 0.36%\n",
            "Epoch 37: Train Loss: 6.5703, Train Acc: 1.47%, Val Loss: 7.6769, Val Acc: 0.36%\n",
            "Epoch 38: Train Loss: 6.5595, Train Acc: 1.34%, Val Loss: 7.6862, Val Acc: 0.40%\n",
            "Epoch 39: Train Loss: 6.5459, Train Acc: 1.32%, Val Loss: 7.6937, Val Acc: 0.40%\n",
            "Epoch 40: Train Loss: 6.5322, Train Acc: 1.46%, Val Loss: 7.7056, Val Acc: 0.36%\n",
            "Epoch 41: Train Loss: 6.5196, Train Acc: 1.30%, Val Loss: 7.7173, Val Acc: 0.36%\n",
            "Epoch 42: Train Loss: 6.5082, Train Acc: 1.41%, Val Loss: 7.7255, Val Acc: 0.40%\n",
            "Epoch 43: Train Loss: 6.4978, Train Acc: 1.41%, Val Loss: 7.7372, Val Acc: 0.36%\n",
            "Epoch 44: Train Loss: 6.4889, Train Acc: 1.58%, Val Loss: 7.7433, Val Acc: 0.40%\n",
            "Epoch 45: Train Loss: 6.4775, Train Acc: 1.53%, Val Loss: 7.7538, Val Acc: 0.36%\n",
            "Epoch 46: Train Loss: 6.4666, Train Acc: 1.60%, Val Loss: 7.7621, Val Acc: 0.36%\n",
            "Epoch 47: Train Loss: 6.4569, Train Acc: 1.55%, Val Loss: 7.7708, Val Acc: 0.40%\n",
            "Epoch 48: Train Loss: 6.4485, Train Acc: 1.49%, Val Loss: 7.7794, Val Acc: 0.44%\n",
            "Epoch 49: Train Loss: 6.4341, Train Acc: 1.60%, Val Loss: 7.7902, Val Acc: 0.44%\n",
            "Epoch 50: Train Loss: 6.4280, Train Acc: 1.60%, Val Loss: 7.7975, Val Acc: 0.44%\n",
            "Validation Accuracy: 0.4439\n",
            "Best Hyperparameters: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Best Validation Accuracy: 0.9321\n"
          ]
        }
      ]
    }
  ]
}