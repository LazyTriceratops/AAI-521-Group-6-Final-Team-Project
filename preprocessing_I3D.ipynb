{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfNET4dfeQ9w"
   },
   "source": [
    "# I3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5171,
     "status": "ok",
     "timestamp": 1733417572360,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "LxZT7KBHeNJY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1733417575426,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "RvA9KvjIfTRs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9495,
     "status": "ok",
     "timestamp": 1733417587091,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "xHfuQbqFfYCc",
    "outputId": "d1c2b7b9-ba9a-4fdc-9c80-dd7f6999f01d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2910,
     "status": "ok",
     "timestamp": 1733417608057,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "xaB-a_jNfUIg"
   },
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "top_100_path = './DataSet/gloss_counts_top_100.csv'\n",
    "SAVE_PATH = \"./DataSet/I3D 100\"\n",
    "\n",
    "# Assuming the uploaded CSV file is named 'top_100_classes.csv'\n",
    "df = pd.read_csv(top_100_path)\n",
    "\n",
    "# Extract the 'Gloss' column for the top 100 classes\n",
    "top_100_classes = df['Gloss'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1668856,
     "status": "ok",
     "timestamp": 1733419279070,
     "user": {
      "displayName": "Aaron Ramirez",
      "userId": "09089953805173967437"
     },
     "user_tz": -540
    },
    "id": "TLi7LjzGfsW4",
    "outputId": "f4cec5b6-e967-4805-fb46-c8b6e1129286"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Function to check memory usage\n",
    "def check_memory():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory usage: {memory.percent}% of {memory.total / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "# Initialize variables\n",
    "sequences = []\n",
    "labels = []\n",
    "batch_size = 5  # Reduced batch size to prevent high memory usage\n",
    "\n",
    "# Start processing the videos\n",
    "for action in tqdm(top_100_classes, desc=\"Loading data for top 100 classes\"):\n",
    "    action_dir = os.path.join(SAVE_PATH, action)\n",
    "\n",
    "    # Check if the directory exists for the action class\n",
    "    if os.path.exists(action_dir):\n",
    "        batch_sequences = []  # Temporary storage for the current batch\n",
    "        batch_labels = []     # Temporary storage for the current batch\n",
    "\n",
    "        for video_file in os.listdir(action_dir):\n",
    "            if video_file.endswith('_frames.npy'):\n",
    "                # Load the processed video frames\n",
    "                video_path = os.path.join(action_dir, video_file)\n",
    "                frames = np.load(video_path)\n",
    "\n",
    "                # Append frames and labels to the current batch\n",
    "                batch_sequences.append(frames)\n",
    "                batch_labels.append(action)\n",
    "\n",
    "                # If batch is full, process it\n",
    "                if len(batch_sequences) == batch_size:\n",
    "                    sequences.extend(batch_sequences)\n",
    "                    labels.extend(batch_labels)\n",
    "\n",
    "                    # Clear the current batch\n",
    "                    batch_sequences = []\n",
    "                    batch_labels = []\n",
    "\n",
    "                    # Check memory usage after processing each batch\n",
    "                    check_memory()\n",
    "                    gc.collect()  # Explicitly clear memory\n",
    "\n",
    "        # Process any remaining videos in the batch\n",
    "        if batch_sequences:\n",
    "            sequences.extend(batch_sequences)\n",
    "            labels.extend(batch_labels)\n",
    "\n",
    "            # Check memory usage after final batch\n",
    "            check_memory()\n",
    "            gc.collect()  # Explicitly clear memory\n",
    "\n",
    "    else:\n",
    "        print(f\"Missing directory for class: {action}\")\n",
    "\n",
    "# Convert sequences and labels to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = [top_100_classes.index(label) for label in labels]\n",
    "y = to_categorical(y, num_classes=len(top_100_classes))  # One-hot encode the labels\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_FKncmjBsod"
   },
   "outputs": [],
   "source": [
    "# Save the data as X_i3d_100.npy and y_i3d_100.npy\n",
    "np.save('./Dataset/final_Dataset/X_i3d_100.npy', X)\n",
    "np.save('./Dataset/final_Dataset/y_i3d_100.npy', y)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNShqWo1hG5RJNRFBXIBA6d",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1KYEW5zkMhtf70XWzkXLa7iK_L0sMjcOr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
